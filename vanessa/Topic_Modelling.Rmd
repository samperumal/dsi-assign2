---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=8, fig.height=5, fig.path='Figs/',
                      echo=TRUE, warning=FALSE, message=FALSE, fig.align = "center" )
```

```{r, echo = TRUE, warning = FALSE}
#Load required librarys
library(tidyr)
library(knitr)
library(tidyverse)
library(dplyr)
library(tidytext)
library(topicmodels)
library(ldatuning)

```


##Topic Modelling


An effective topic model can sumamrise the ideas and concets within a docuemnts - this can be used in varous ways.  A user can understand the main themes within the corpus of documents and draw conculisions from these from simple analysis or can use the infomation as type of dimesional reduction and feed these topics into different supervised or unsupervised algorthims.

In this project, our group has used topic modelling to better understand the common topics that come up over the SONA spechs, how these are realted to different presidents and speeches and how they change over time.  In addtion, we have used these tp help classify which sentence was said by which president (see Section XX)

###Data

The data used in this section is the clean and processed data as described in Section X.


###Methodolgy

The following methodoly was followed:

1. Each sentence was tokeniseised into "bigrams", stop words removed and a document-term matrix set up
    Bigrams were chosen over idvidual words as they provided more context and meaning.
2. Four optimisation techniques were used to help determine the number of topics that are covered in the corpus of documents
3. Latent Dirichlet allocation was used to determine the probability of bigrams belong to certain topics and the probability that sentences belonged to topics.
4. Text mining methods were delpyed to extract insight into the different topics
5. The probability of sentences to each topics were then passed through to a neural network.

####Step One: Tokenisation, Remove Stop words and Document Term Matrix

```{r, echo = TRUE}

#Load data
load("../input_data.RData")

inputdata <- input_data$sentences

#Getting Bigrams out
tidy_sona3 = input_data$sentences %>%  unnest_tokens(bigram, sentence, token = "ngrams", n  = 2)

#Get stop words
data("stop_words")

# separate the bigrams 
tidy_sona3_sep <- tidy_sona3 %>%
  separate(bigram, c("word1", "word2"), sep = " ")

# remove stop words
bigrams_filtered <- tidy_sona3_sep %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# join up the bigrams again
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")


#create term count to feed into DTM
bigrams_count <- bigrams_united %>%
  group_by(id, bigram) %>%
  summarise(count_bigram = n()) %>%
  select(id, bigram, count_bigram) %>%
  arrange(desc(count_bigram))


#create document term matrix (DTM)
dtm_grams <- bigrams_count %>% 
cast_dtm(id, bigram, count_bigram)


#Lets see what the most commonly used terms are
bigrams_count_plot <- bigrams_united %>%
  group_by(bigram) %>%
  summarise(count_bigram = n()) %>%
  select(bigram, count_bigram) %>%
  arrange(desc(count_bigram))


bigrams_count_plot %>%
  top_n(20, count_bigram) %>%
  ungroup() %>%
  arrange(desc(count_bigram)) %>%
ggplot(aes(bigram, count_bigram)) +
  geom_col(show.legend = FALSE , color = "blue", fill = "blue") +
  coord_flip()


```


After tokinisation and removal of stop words, the top 20 most used terms across all of the SONA speeches are displayed.  Unsurpisringly, "South Africa" is the most used terms followed closly by "South African" and"South Africans".  "Public service and "Local Goverment" are then the ost used terms.

###Step 2: Optimisation of k - the number of topics.

A pre-requiste of topic modelling is understanding the number of topics (latent factor k) that each corpus may contain.  In some cases, this may be a fair assumption but as in our case, how would we, without reading though each speech, know how many different topics would be discussed?  Luckily, Murzintcev Nikita has published a R- package (ldatuning) that helps to optimise the number of topics(k) over four different measure. The three measures used to determine the number of topics, are discussed in an RPubs paper which can found here: http://www.rpubs.com/MNidhi/NumberoftopicsLDA and the following otpimisation largely follows the accomapying vingette: https://cran.r-project.org/web/packages/ldatuning/vignettes/topics.html

*Extract from RPubs*

"*Arun2010: The measure is computed in terms of symmetric KL-Divergence of salient distributions that are derived from these matrix factor and is observed that the divergence values are higher for non-optimal number of topics (maximize)*

*CaoJuan2009: method of adaptively selecting the best LDA model based on density.(minimize)*

*Griffths: To evaluate the consequences of changing the number of topics T, used the Gibbs sampling algorithm to obtain samples from the posterior distribution over z at several choices of T(minimize)*"



```{r, }

bigrams_sample <- bigrams_count %>% ungroup()

n =nrow(bigrams_sample)

index = sample(1:nrow(bigrams_sample), round(n*0.8,0))


train_data <- bigrams_sample[index,]
test_data <- bigrams_sample[-index,]



dtm_train <- train_data %>% 
cast_dtm(id, bigram, count_bigram)

dtm_test <- test_data %>% 
cast_dtm(id, bigram, count_bigram)

result <- FindTopicsNumber(
  dtm_train,
  topics = seq(from = 2, to = 15, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = FALSE
)

FindTopicsNumber_plot(result)

```

From the above plot, the marginal benefit from adding another topic, stops at around 10 topics.  In order to test this, we can check the *"perplexity"* over a test sampe for the document term matrix (which occrding to Nikita *"measures the log-likelihood of a held-out test set; Perplexity is a measurement of how well a probability distribution or probability model predicts a sample"*)


```{r}
perplexity_df <- data.frame(train=numeric(), test=numeric())
topics <- c(2:15)
burnin = 100
iter = 1000
keep = 50

set.seed(12345)
for (i in topics){
 
  fitted <- LDA(dtm_train, k = i, method = "Gibbs",
                control = list(burnin = burnin, iter = iter, keep = keep) )
  perplexity_df[i,1] <- perplexity(fitted, newdata = dtm_train)
  perplexity_df[i,2]  <- perplexity(fitted, newdata = dtm_test) 
}


##plotting the perplexity of both train and test

g <- ggplot(data=perplexity_df, aes(x= as.numeric(row.names(perplexity_df)))) + labs(y="Perplexity",x="Number of topics") + ggtitle("Perplexity of hold out  and training data")

g <- g + geom_line(aes(y=test), colour="red")
g <- g + geom_line(aes(y=train), colour="green")
g 
```







<!-- ```{r} -->
<!-- ###Topic 1 -->

<!-- top_terms.1 <- speech_topics %>% -->
<!--   filter(topic == "topic1") %>% -->
<!--   group_by(topic) %>% -->
<!--   top_n(50, beta) %>% -->
<!--   ungroup() %>% -->
<!--   arrange(topic, -beta)  -->

<!-- plot_topic_1 <- top_terms.1 %>% -->
<!--   mutate(Bigrams = reorder(Bigrams, beta)) %>% -->
<!--   ggplot(aes(Bigrams, beta, fill = factor(topic))) + -->
<!--   geom_col(show.legend = FALSE) + -->
<!--   facet_wrap(~ topic, scales = "free") + -->
<!--   coord_flip() -->


<!-- word_count.1 <- top_terms.1 %>% -->
<!--   filter(topic == "topic1") %>% -->
<!--   mutate(freq = round(scale(beta*1000),0) + 1) %>% -->
<!--   select(Bigrams, freq) -->


<!-- wordcloud.1 <- wordcloud2(word_count.1, color = "random-dark") -->

<!-- par(mfrow= c(2,1)) -->
<!-- plot_topic_1 -->
<!-- wordcloud.1 -->

<!-- #  -->
<!-- # ##Topic 1 an 2 -->
<!-- # small_topic <- speech_topics %>% -->
<!-- #   filter(topic == "topic1"|topic == "topic2") -->
<!-- #  -->
<!-- # beta_spread <- small_topic %>% -->
<!-- # #  mutate(topic = paste0("topic", topic)) %>% -->
<!-- #   spread(topic, beta) %>% -->
<!-- #   filter(topic1 > .001 | topic2 > .001) %>% -->
<!-- #   mutate(log_ratio = log2(topic1 / topic2)) -->
<!-- #  -->
<!-- # beta_spread %>% -->
<!-- #   group_by(direction = log_ratio > 0) %>% -->
<!-- #   top_n(10, abs(log_ratio)) %>% -->
<!-- #   ungroup() %>% -->
<!-- #   mutate(Bigrams = reorder(Bigrams, log_ratio)) %>% -->
<!-- #   ggplot(aes(Bigrams, log_ratio)) + -->
<!-- #   geom_col() + -->
<!-- #   labs(y = "Log2 ratio of beta in topic 1 / topic 2") + -->
<!-- #   coord_flip() -->
<!-- #  -->
<!-- #  -->
<!-- # #Topic 1 and 3 -->
<!-- #  -->
<!-- # small_topic <- speech_topics %>% -->
<!-- #   filter(topic == "topic1"|topic == "topic3") -->
<!-- #  -->
<!-- # beta_spread <- small_topic %>% -->
<!-- # #  mutate(topic = paste0("topic", topic)) %>% -->
<!-- #   spread(topic, beta) %>% -->
<!-- #   filter(topic1 > .001 | topic3 > .001) %>% -->
<!-- #   mutate(log_ratio = log2(topic1 / topic3)) -->
<!-- #  -->
<!-- # beta_spread %>% -->
<!-- #   group_by(direction = log_ratio > 0) %>% -->
<!-- #   top_n(10, abs(log_ratio)) %>% -->
<!-- #   ungroup() %>% -->
<!-- #   mutate(Bigrams = reorder(Bigrams, log_ratio)) %>% -->
<!-- #   ggplot(aes(Bigrams, log_ratio)) + -->
<!-- #   geom_col() + -->
<!-- #   labs(y = "Log2 ratio of beta in topic 1 / topic 3") + -->
<!-- #   coord_flip() -->


<!-- ``` -->


```{r}
###Topic 2

# top_terms.2 <- speech_topics %>%
#   filter(topic == "topic2") %>%
#   group_by(topic) %>%
#   top_n(50, beta) %>%
#   ungroup() %>%
#   arrange(topic, -beta) 
# 
# plot_topic_2 <- top_terms.2 %>%
#   mutate(Bigrams = reorder(Bigrams, beta)) %>%
#   ggplot(aes(Bigrams, beta, fill = factor(topic))) +
#   geom_col(show.legend = FALSE) +
#   facet_wrap(~ topic, scales = "free") +
#   coord_flip()
# 
# 
# word_count.2 <- top_terms.2 %>%
#   filter(topic == "topic2") %>%
#   mutate(freq = round(scale(beta*1000),0) + 1) %>%
#   select(Bigrams, freq)
# 
# 
# wordcloud.2 <- wordcloud2(word_count.2, color = "random-dark")
# 
# par(mfrow= c(2,1))
# plot_topic_2
# wordcloud.2

```


```{r}

```

