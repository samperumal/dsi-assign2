---
title: "R Notebook"
output:
  html_document:
    df_print: paged
always_allow_html: yes
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=8, fig.height=5, fig.path='Figs/',
                      echo=TRUE, warning=FALSE, message=FALSE, fig.align = "center" )
```

```{r, echo = TRUE, warning = FALSE}
#Load required librarys
library(tidyr)
library(knitr)
library(tidyverse)
library(dplyr)
library(tidytext)
library(topicmodels)
library(ldatuning)
library(wordcloud2)



```

##Topic Modelling


An effective topic model can summarise the ideas and concepts within a document - this can be used in various ways.  A user can understand the main themes within the corpus of documents and draw conclusions from these from analysis of these topics or they can use the information as type of dimensional reduction and feed these topics into different supervised or unsupervised algorithms.

In this project, our group has used topic modelling to better understand the common topics that come up over the SONA speeches, how these are related to different presidents and speeches and how they change over time.  In addition, the probability that a sentence belongs to a certain topic was used in an attempt to classify which sentence was said by which president (see Section XX)

###Data

The data used in this section is the clean and processed data as described in Section X.  The resulting sentence data has been used and dissected further without consideration to train and validation unless otherwise stated. 


###Methodology

The following methodology was followed:

1. Each sentence was tokenised into "bigrams", stop words removed and a document-term matrix set up
    Bigrams were chosen over individual words as they provided more context and meaning.
2. An optimisation technique was used to help determine the number of topics that are covered in the corpus of documents and this optimisation was validated on a hold out sample.
3. Latent Dirichlet allocation was used to determine the probability of bigrams belong to certain topics and the probability that sentences belonged to topics.
4. Text mining methods were deployed to extract insight into the different topics
5. The probability of sentences to each topics were then passed through to a neural network.

####Step One: Tokenisation, Remove Stop words and Document Term Matrix


```{r, echo = TRUE}

#Load data
load("../input_data.RData")
#Get sentence data
inputdata <- input_data$sentences

#Getting Bigrams out
tidy_sona3 = input_data$sentences %>%  unnest_tokens(bigram, sentence, token = "ngrams", n  = 2)

#Get stop words
data("stop_words")

# separate the bigrams 
tidy_sona3_sep <- tidy_sona3 %>%
  separate(bigram, c("word1", "word2"), sep = " ")

# remove stop words
bigrams_filtered <- tidy_sona3_sep %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# join up the bigrams again
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")


#Lets see what the most commonly used terms are
bigrams_count_plot <- bigrams_united %>%
  group_by(bigram) %>%
  summarise(count_bigram = n()) %>%
  select(bigram, count_bigram) %>%
  arrange(desc(count_bigram))
#plot these top terms
bigrams_count_plot %>%
  top_n(20, count_bigram) %>%
  ungroup() %>%
  arrange(desc(count_bigram)) %>%
ggplot(aes(bigram, count_bigram)) +
  geom_col(show.legend = FALSE , color = "blue", fill = "blue") +
  coord_flip()



```

After tokenisation and removal of stop words, the top 20 most used terms across all of the SONA speeches are displayed.  Unsurprisingly, "South Africa" is the most used term followed closely by "South African" and "South Africans".  These terms do not add to our understanding of the topics and tend to confuse the topic modelling going forward.  The removal of the terms allows for a cleaner interpretation.  "Public service and "Local Government" are then the most used terms.


```{r}

#create term count to feed into DTM
bigrams_count <- bigrams_united %>%
  group_by(id, bigram) %>%
  summarise(count_bigram = n()) %>%
  select(id, bigram, count_bigram) %>%
  arrange(desc(count_bigram))


#remove the terms from the data set
bigrams_count <- bigrams_count %>% 
    filter (bigram != "south africa") %>%
    filter (bigram != "south african") %>%
    filter (bigram != "south africans") %>%
      filter (bigram != "local government")


#create document term matrix (DTM)
dtm_grams <- bigrams_count %>% 
cast_dtm(id, bigram, count_bigram)


```



###Step 2: Optimisation of k - the number of topics.

A pre-requisite of topic modelling is knowing the number of topics that each corpus may contain (i.e. the latent factor k) In some cases, this may be a fair assumption but without reading though each speech, how  could one know how many different topics have been articulated in the SONA's?  Luckily, Murzintcev Nikita has published an R- package (ldatuning) that helps to optimise the number of topics (k) over three different measures. The measures used to determine the number of topics, are discussed in an RPubs paper which can found here: http://www.rpubs.com/MNidhi/NumberoftopicsLDA and the following optimisation largely follows the accompanying vignette: https://cran.r-project.org/web/packages/ldatuning/vignettes/topics.html

The following extract from the RPub paper gives a brief explanation of the methods used to optimise for k:

*Extract from RPubs*

"*Arun2010: The measure is computed in terms of symmetric KL-Divergence of salient distributions that are derived from these matrix factor and is observed that the divergence values are higher for non-optimal number of topics (maximize)*

*CaoJuan2009: method of adaptively selecting the best LDA model based on density.(minimize)*

*Griffths: To evaluate the consequences of changing the number of topics T, used the Gibbs sampling algorithm to obtain samples from the posterior distribution over z at several choices of T(minimize)*"


In addition to this, Nikita considers how the number of k may change over a validation or hold out sample.  His term for this is*"perplexity"* which he defines as *"[it] measures the log-likelihood of a held-out test set; Perplexity is a measurement of how well a probability distribution or probability model predicts a sample"*

Below is an attempt to optimise for k and to check that the choice of k holds over an unseen data set.

```{r, }
#sepertate the bigrams into a train and test set
#ungroup the daat
bigrams_sample <- bigrams_count %>% ungroup()
#number of rows
n =nrow(bigrams_sample)
#sample from data
index = sample(1:nrow(bigrams_sample), round(n*0.8,0))

#seperate into test and train
train_data <- bigrams_sample[index,]
test_data <- bigrams_sample[-index,]


#put train and test into the DTM matrix formats
dtm_train <- train_data %>% 
cast_dtm(id, bigram, count_bigram)

dtm_test <- test_data %>% 
cast_dtm(id, bigram, count_bigram)

#optimise for k
result <- FindTopicsNumber(
  dtm_train,
  topics = seq(from = 2, to = 15, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = FALSE
)
#plot the measures
FindTopicsNumber_plot(result)

```

From the above plot, the marginal benefit from adding another topic, stops at around 8-10 topics.  In order to test this, the *"perplexity"* over a test sample for the document term matrix can be checked.

```{r}
#initalise loop
perplexity_df <- data.frame(train=numeric(), test=numeric())
#set patamaets
topics <- c(2:15)
burnin = 100
iter = 1000
keep = 50

#loop to test over train and test
set.seed(12345)
for (i in topics){
 
  fitted <- LDA(dtm_train, k = i, method = "Gibbs",
                control = list(burnin = burnin, iter = iter, keep = keep) )
  perplexity_df[i,1] <- perplexity(fitted, newdata = dtm_train)
  perplexity_df[i,2]  <- perplexity(fitted, newdata = dtm_test) 
}


##plotting the perplexity of both train and test

g <- ggplot(data=perplexity_df, aes(x= as.numeric(row.names(perplexity_df)))) + labs(y="Perplexity",x="Number of topics") + ggtitle("Perplexity of hold out and training data") + scale_x_discrete(limits = topics)

g <- g + geom_line(aes(y=test), colour="red")
g <- g + geom_line(aes(y=train), colour="blue")
g 
```

As more topics are used, the perplexity of the training sample does decrease but that of the test sample increases from around 11 topic.  The perplexity of the test sample seems to be minimised at around 8 topics.  

The evidence from these two plots suggest that the optimal number topics sits at around 8 topics.  

###Step 3: Latent Dirichlet allocation 


For this assignment, Latent Dirichlet allocation (LDA) was used for the topic modelling.  Other methods, such as a Latent Semantic Analysis (LSA) or Probabilistic Latent Semantic Analysis (pLSA) could have been used but LDA is useful due to the fact that it allows:
      1. Each document within the corpus to be a mixture of topics
      2. Each topic to be a mixture of bigrams
      3. The topics are assumed to be drawn from Dirichlet distribution (i.e. not k different distributions as with pLSA) so there are less parameters to estimate and no need to estimate the probability that the corpus generates a specific document.
      
      
      
```{r}

##chose 5 topics
#set seed
set.seed(1234)
#run teh LDA with full dtm matrix
topics_lda <- LDA(dtm_grams, k = 5, control = list(seed = 1234))

#get ther bigrams out from model
term <- as.character(topics_lda@terms)
#bind the betas and the topics together
speech_topics <- as.tibble(cbind(term, t(topics_lda@beta)))
#lables for the data
colnames(speech_topics) <- c("Bigrams", "topic1","topic2","topic3","topic4","topic5")

#gathers into tidy format 
#note a term should appear more than once.
speech_topics <- speech_topics %>% 
  gather(topic1
         , topic2
         , topic3
         , topic4
         , topic5
         , key = "topic", value = "beta")


##mutate the betas
speech_topics <- speech_topics %>%
  mutate(beta = as.numeric(beta)) %>%
  mutate(beta = exp(beta))


```
      
The beta matrix produced gives the probablity of the topic prodcusing that bigram (i.e. that the phrase is in reference to that topic.)  From this measure, one can get a sense of what the topic is about. By using the most popular phrases in each topic, understanding of the flavour of each topic.  However, it must be kept in mind that terms can belong to *more than one topic* so applying some logic to get a theme or flavour should e applied liberally.

####Topic One

From the display of popular terms, it can be determined that the topic one has a vaugue connection to "job creation" - this is the most common terms but is supported by other terms that have a high proabbility of being generated by this topic such as: 
+ world cup
+ national youth
+ infratsrticutere develpoment

These concepts all support the idea of the job creation as each of these will generate jobs for the country. But there is some noise in the topic for "adress terms" i.e. houurable pseaker or hounrable chairperson.  "Nelson Mandela" and "Presdient Mandela" crop up too which suggests that along side the job creation theme, there exists some of what can be termed "terms of endearmnt" 

```{r}
###Topic 1

#filter for topic one
top_terms.1 <- speech_topics %>%
  filter(topic == "topic1") %>%
  group_by(topic) %>%
  top_n(30, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
##plot the most popluar terms
plot_topic_1 <- top_terms.1 %>%
  mutate(Bigrams = reorder(Bigrams, beta)) %>%
  ggplot(aes(Bigrams, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

#set up for word cloud
maxs <- max((top_terms.1$beta*100))
mins <- min(top_terms.1$beta*100)


word_count.1 <- top_terms.1 %>%
  mutate(freq = scale(beta*100, center = mins, scale = maxs - mins)) %>%
  select(Bigrams, freq)



#word cloud
set.seed(12345)
wordcloud.1 <- wordcloud2(word_count.1, color = "random-dark", size = .4)

par(mfrow= c(2,1))
plot_topic_1
set.seed(12345)
wordcloud.1


```


####Topic Two


As with the previous topic, there is some random "terms of endearment" in this topic as well (i.e. madame speaker) but it is not as evident as in the first topic.  The next four terms sums out the main themes for this topic:
+ Economic Empowerment
+ Black Economic
+ Justce System
+ Criminal Jutice

In summary, this topic can be summed as "Economy/ Criminal and Justice system"


```{r}
###Topic 2

#filter for topic two
top_terms.2 <- speech_topics %>%
  filter(topic == "topic2") %>%
  group_by(topic) %>%
  top_n(30, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
##plot the most popluar terms
plot_topic_2 <- top_terms.2 %>%
  mutate(Bigrams = reorder(Bigrams, beta)) %>%
  ggplot(aes(Bigrams, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

#set up for word cloud
maxs <- max((top_terms.2$beta*100))
mins <- min(top_terms.2$beta*100)


word_count.2 <- top_terms.2 %>%
  mutate(freq = scale(beta*100, center = mins, scale = maxs - mins)) %>%
  select(Bigrams, freq)



#word cloud
set.seed(12345)
wordcloud.2 <- wordcloud2(word_count.2, color = "random-dark", size = .5)


par(mfrow= c(2,1))
plot_topic_2
#set.seed(12345)
wordcloud.2






```


####Topic Three


Despite the most popular terms being "United Nation" and "private sector", there a theme that is "develpoing". As in develpoment plan, resource development, national development and development programme etc.  And thus, the topic is named.

```{r}
###Topic 3

#filter for topic three
top_terms.3 <- speech_topics %>%
  filter(topic == "topic3") %>%
  group_by(topic) %>%
  top_n(30, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
##plot the most popluar terms
plot_topic_3 <- top_terms.3 %>%
  mutate(Bigrams = reorder(Bigrams, beta)) %>%
  ggplot(aes(Bigrams, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

#set up for word cloud
maxs <- max((top_terms.3$beta*100))
mins <- min(top_terms.3$beta*100)


word_count.3 <- top_terms.3 %>%
  mutate(freq = scale(beta*100, center = mins, scale = maxs - mins)) %>%
  select(Bigrams, freq)



#word cloud
set.seed(12345)
wordcloud.3 <- wordcloud2(word_count.3, color = "random-dark", size = .3)


par(mfrow= c(2,1))
plot_topic_3
wordcloud.3



```


###Topic 4

Once again, there is a "term of endeament" in the popular terms ("fellow south" which is assumed short for "fellow South African's" which is one of former President Zuma's favourite phrases).  With all the other terms combined, a theme of "Social Reform and Regional/Munciple Goverment" takes shape.

Given that there is a possible trigram evident here, it may be worth exploring in future work.

``` {r}

#filter for topic four
top_terms.4 <- speech_topics %>%
  filter(topic == "topic4") %>%
  group_by(topic) %>%
  top_n(30, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
##plot the most popluar terms
plot_topic_4 <- top_terms.4 %>%
  mutate(Bigrams = reorder(Bigrams, beta)) %>%
  ggplot(aes(Bigrams, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

#set up for word cloud
maxs <- max((top_terms.4$beta*100))
mins <- min(top_terms.4$beta*100)


word_count.4 <- top_terms.4 %>%
  mutate(freq = scale(beta*100, center = mins, scale = maxs - mins)) %>%
  select(Bigrams, freq)



#word cloud
set.seed(12345)
wordcloud.4 <- wordcloud2(word_count.4, color = "random-dark", size = .3)

par(mfrow = c(1,2))
plot_topic_4
wordcloud.4



```



###Topic 5

"Public Sector", "local goverment" and "private sector" are popular terms in topic 5.  After consideratio of the various other terms, of which some have cross over with other topics and dicussion, the eventual name for this topic becaome "Public Sector Entities"



``` {r}

#filter for topic five
top_terms.5 <- speech_topics %>%
  filter(topic == "topic5") %>%
  group_by(topic) %>%
  top_n(30, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
##plot the most popluar terms
plot_topic_5 <- top_terms.5 %>%
  mutate(Bigrams = reorder(Bigrams, beta)) %>%
  ggplot(aes(Bigrams, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

#set up for word cloud
maxs <- max((top_terms.5$beta*100))
mins <- min(top_terms.5$beta*100)


word_count.5 <- top_terms.5 %>%
  mutate(freq = scale(beta*100, center = mins, scale = maxs - mins)) %>%
  select(Bigrams, freq)



#word cloud
set.seed(12345)
wordcloud.5 <- wordcloud2(word_count.5, color = "random-dark", size = .6)
#saveWidget(wordcloud.5,"5.html",selfcontained = F)

par(mfrow= c(2,1))
plot_topic_5
wordcloud.5



```

A differetn way of looking at this topic could be to investigate the biggest differenatil in terms between topics.  For instance, using the log(base 2) ratio between topic 1 and topic 5, show the terms that have the widest margin between the two topic (i.e. are far more likely to be in topic 5 versus topic 1)


```{r}
#comparison of topic 1 to topic 5

small_topic <- speech_topics %>%
  filter(topic == "topic5"|topic == "topic1")


beta_spread <- small_topic %>%
  #  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic5 > .001) %>%
  mutate(log_ratio = log2(topic5 / topic1))

beta_spread %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(Bigrams = reorder(Bigrams, log_ratio)) %>%
  ggplot(aes(Bigrams, log_ratio)) +
  geom_col() +
  labs(y = "Log2 ratio of beta in topic 5 / topic 1") +
  coord_flip()



```


For instance, "social programmes", "human fufliment" , "rights comminsion" are all genetated in signifigantly larger proportions by Topic 5 compared to Topic 1 while "natonal social", "training collegues" and "sector unions" all exist with in Topic 1.

Given the naming of Topic 5 as "Pblic Sector entities" and Topic 1 as "Job creation and terms of endearment" these terms do seem to be grouped in line with expectation.






```{r}
#comparison of topic 4 to topic 5

small_topic <- speech_topics %>%
  filter(topic == "topic5"|topic == "topic4")


beta_spread <- small_topic %>%
  #  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic4 > .001 | topic5 > .001) %>%
  mutate(log_ratio = log2(topic5 / topic4))

beta_spread %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(Bigrams = reorder(Bigrams, log_ratio)) %>%
  ggplot(aes(Bigrams, log_ratio)) +
  geom_col() +
  labs(y = "Log2 ratio of beta in topic 5 / topic 4") +
  coord_flip()



```



```{r}

#comparison of topic 2 to topic 5

small_topic <- speech_topics %>%
  filter(topic == "topic5"|topic == "topic2")


beta_spread <- small_topic %>%
  #  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic2 > .001 | topic5 > .001) %>%
  mutate(log_ratio = log2(topic5 / topic2))

beta_spread %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(Bigrams = reorder(Bigrams, log_ratio)) %>%
  ggplot(aes(Bigrams, log_ratio)) +
  geom_col() +
  labs(y = "Log2 ratio of beta in topic 5 / topic 2") +
  coord_flip()



```