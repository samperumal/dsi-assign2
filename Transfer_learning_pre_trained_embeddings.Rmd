---
title: "Pre-trained embeddings (transfer learning)"
author: "Merve AYDIN CHESTER"
date: "9/19/2018"
output: html_document
---

```{r setup, include=FALSE}
##---------------------------------##
## ADD ANY LIBRARIES USED HERE
##---------------------------------##

knitr::opts_chunk$set(
  echo = TRUE,
  library(tidyverse),
  library(tidytext),
  library(stringr),
  library(knitr),
  library(keras))
```

```{r}


#Load the data we are going to be embedding
load("~/Downloads/dsi-assign2/input_data.RData")
load("~/Downloads/dsi-assign2/balanced_train_data.RData")
load("~/Downloads/dsi-assign2/sentence_data.RData")

#Settings for gloves embedding
GLOVE_DIR <- 'glove.6B'
MAX_SEQUENCE_LENGTH <- 1000
MAX_NUM_WORDS <- 40000
EMBEDDING_DIM <- 100
VALIDATION_SPLIT <- 0.2

# download data if necessary
download_data <- function(data_dir, url_path, data_file) {
  if (!dir.exists(data_dir)) {
    download.file(paste0(url_path, data_file), data_file, mode = "wb")
    if (tools::file_ext(data_file) == "zip")
      unzip(data_file, exdir = tools::file_path_sans_ext(data_file))
    else
      untar(data_file)
    unlink(data_file)
  }
}
download_data(GLOVE_DIR, 'http://nlp.stanford.edu/data/', 'glove.6B.zip')

# first, build index mapping words in the embeddings set
# to their embedding vector

cat('Indexing word vectors.\n')

embeddings_index <- new.env(parent = emptyenv())
lines <- readLines(file.path(GLOVE_DIR, 'glove.6B.100d.txt'))
for (line in lines) {
  values <- strsplit(line, ' ', fixed = TRUE)[[1]]
  word <- values[[1]]
  coefs <- as.numeric(values[-1])
  embeddings_index[[word]] <- coefs
}

cat(sprintf('Found %s word vectors.\n', length(embeddings_index)))

# second, prepare text samples and their labels
cat('Tokenizing sona sentences\n')


# finally, vectorize the text samples into a 2D integer tensor
tokenizer <- text_tokenizer(num_words=MAX_NUM_WORDS)
tokenizer %>% fit_text_tokenizer(input_data$sentences$sentence)

# save the tokenizer in case we want to use it again
# for prediction within another R session, see:
# https://keras.rstudio.com/reference/save_text_tokenizer.html
save_text_tokenizer(tokenizer, "tokenizer")

# sequences <- texts_to_sequences(tokenizer, input_data$sentences$sentence)
sequences_train = texts_to_sequences(tokenizer, balanced_train_data$sentence)
sequences_valid = texts_to_sequences(tokenizer, sentence_data$validate$sentence)

word_index <- tokenizer$word_index
cat(sprintf('Found %s unique tokens.\n', length(word_index)))

x_train <- pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)
x_val <- pad_sequences(sequences_valid, maxlen=MAX_SEQUENCE_LENGTH)

# Fit president tokenizer
president_count = balanced_train_data$president %>% as_tibble() %>% unique() %>% count()
response_tokenizer = text_tokenizer(num_words = president_count + 1)
response_tokenizer$fit_on_texts(balanced_train_data$president)
response_tokenizer$fit_on_texts(sentence_data$validate$president )

# One-hot encode president
# Extract response vector, ignoring first (empty) column
y_train = (response_tokenizer$texts_to_matrix(balanced_train_data$president, mode = "binary"))[,-1]
y_val = (response_tokenizer$texts_to_matrix(sentence_data$validate$president , mode = "binary"))[,-1]

cat('Shape of data tensor: ', dim(x_train), '\n')
cat('Shape of data tensor: ', dim(x_val), '\n')
cat('Shape of label tensor: ', dim(y_train), '\n')
cat('Shape of data tensor: ', dim(y_val), '\n')


# prepare embedding matrix
num_words <- min(MAX_NUM_WORDS, length(word_index) + 1)
prepare_embedding_matrix <- function() {
  embedding_matrix <- matrix(0L, nrow = num_words, ncol = EMBEDDING_DIM)
  for (word in names(word_index)) {
    index <- word_index[[word]]
    if (index >= MAX_NUM_WORDS)
      next
    embedding_vector <- embeddings_index[[word]]
    if (!is.null(embedding_vector)) {
      # words not found in embedding index will be all-zeros.
      embedding_matrix[index,] <- embedding_vector
    }
  }
  embedding_matrix
}

embedding_matrix <- prepare_embedding_matrix()

# load pre-trained word embeddings into an Embedding layer
# note that we set trainable = False so as to keep the embeddings fixed
embedding_layer <- layer_embedding(
  input_dim = num_words,
  output_dim = EMBEDDING_DIM,
  weights = list(embedding_matrix),
  input_length = MAX_SEQUENCE_LENGTH,
  trainable = FALSE
)

cat('Training model\n')

# train a 1D convnet with global maxpooling
sequence_input <- layer_input(shape = list(MAX_SEQUENCE_LENGTH), dtype='int32')

preds <- sequence_input %>%
  embedding_layer %>% 
  layer_conv_1d(filters = 64, kernel_size = 5, activation = 'relu',
                use_bias = TRUE, kernel_initializer = "glorot_uniform") %>% 
  layer_dropout(0.2) %>%
  layer_max_pooling_1d(pool_size = 5) %>% 
  layer_conv_1d(filters = 64, kernel_size = 5, activation = 'relu') %>% 
  layer_max_pooling_1d(pool_size = 5) %>% 
  layer_conv_1d(filters = 64, kernel_size = 5, activation = 'relu') %>% 
  layer_max_pooling_1d(pool_size = 35) %>% 
  layer_flatten() %>% 
  layer_dense(units = 64, activation = 'relu') %>% 
  layer_dense(units = president_count, activation = 'softmax')


model <- keras_model(sequence_input, preds)

model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(lr=0.002),
  metrics = c('acc')  
)

model %>% fit(
  x_train, y_train,
  batch_size = 128,
  epochs = 15,
  validation_data = list(x_val, y_val)
) %>% plot()

#If the validation loss was still decreasing and the accuracy was increasing we could run 5 more epochs
# model %>% 
#   fit(x_train, y_train, epochs = 5, batch_size = 128,validation_data = list(x_val, y_val)) %>% 
#   plot()


predictions <- model %>% predict(x_val)
range(predictions)


#Getting the max of each rows prediction and binding it on the end
pres.max <- as.data.frame(predictions)
pres.max <- apply(predictions,1,which.max) 
pres.max <- unlist(as.numeric(as.character(pres.max)))
pres.max <- cbind(predictions,pres.max )

pres.actual <- as.data.frame(y_val)
pres.actual <- apply(y_val,1,which.max) 


predictions_val <- cbind(sentence_data$validate,pres.max,pres.actual)
View(head(predictions_val, n= 100))
predictions_val %>% filter(president == "Mbeki")

t = table(predictions_val$pres.max, predictions_val$pres.actual)


```

