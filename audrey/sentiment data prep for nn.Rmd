---
title: "Sentiment Data Prep for NN"
author: "Audrey Pentz"
date: "September 15, 2018"
output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(
  echo = TRUE,
  library(tidyverse),
  library(tidytext),
  library(stringr),
  library(lubridate),
  library(knitr)
)

```



## **Read in the Data**

```{r import}

txt_files <- list.files("../sona-text-1994-2018/")

sona <- data.frame(filename = as.character(), speech = as.character())
for(i in txt_files){
  file_name <- paste0("../sona-text-1994-2018/", i)
  
  # import text as single character string (can also read.table but the "seperator" causes problems)
  this_speech <- readChar(file_name, 
                          nchars = file.info(file_name)$size)
  
  # make data frame with metadata (filename contains year and pres) and speech
  this_sona <- data.frame(filename = i, speech = this_speech, stringsAsFactors = FALSE)
  
  # make a single dataset
  sona <- rbind(sona, this_sona)
}

# extract year
sona$year <- str_sub(sona$filename, start = 1, end = 4)

# extract president name
sona$president <- unlist(str_extract_all(sona$filename, '(?<=_)[^_]+(?=.txt)'))
# this finds everything between "_" and ".txt"

```



## **Remove Stop Words and Tokenize by Sentences, Words and Bigrams**

```{r tokenize}

# sentence tokenization
tidy_sona_sentences <- sona %>% 
  unnest_tokens(text, speech, token = "sentences")
# add an ID variable for sentences
tidy_sona_sentences <- tidy_sona_sentences %>%
  mutate(sentence_id = row_number() )

# word tokenization
tidy_sona_words <- tidy_sona_sentences %>% 
  unnest_tokens(word, text, token = "words") %>% 
  filter(!word %in% stop_words$word, str_detect(word, "[a-z]"))  # remove stop words

# bigram tokenization
bigrams <- tidy_sona_sentences %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
# separate the bigrams 
bigrams_separated <- bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")
# remove stop words
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)
# join up the bigrams again
tidy_sona_bigrams <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

```



## **Sentiment Analysis using "bing" lexicon ("positive" or "negative")**

```{r bing}

# bing sentiment per sentence
bing_sentiment <- tidy_sona_words %>%
  left_join(get_sentiments("bing")) %>%
  group_by(filename, sentence_id) %>% 
  count(sentiment) %>%
  mutate(sentiment = ifelse(is.na(sentiment), "not_in_bing_lexicon", sentiment)) %>% 
  spread(sentiment, n, fill = 0) %>%
  mutate(bing_sentiment = positive - negative,
         total_words = positive + negative + not_in_bing_lexicon,
         bing_sentiment_perc = round((bing_sentiment/(total_words)*100),2)) %>% 
         # bing sentiment as a percentage of total words in the sentence
 select(-positive, -negative)

```


## **Sentiment Analysis using "afinn" lexicon (scale from -5 negative to +5 positive)**

```{r afinn}

# afinn sentiment per sentence
afinn_sentiment <- tidy_sona_words %>%
  left_join(get_sentiments("afinn")) %>%
  group_by(filename, sentence_id) %>% 
  count(score) %>% 
  mutate(score = ifelse(is.na(score), 0, score),
         total_words = sum(n),
         afinn_sentiment = sum(score * n),   # weighted score
         afinn_sentiment_perc = round((afinn_sentiment/total_words*100),2)) %>% 
         # afinn sentiment as a percentage of total words in the sentence
  spread(key = score, value = n, fill = 0)

```


## **Sentiment Analysis using "nrc" lexicon (infers emotion with certain words)**

```{r nrc}

# nrc sentiment per sentence
nrc_sentiment <- tidy_sona_words %>%
  left_join(get_sentiments("nrc")) %>%
  group_by(filename, sentence_id) %>% 
  count(sentiment) %>% 
  mutate(sentiment = ifelse(is.na(sentiment), "not_in_nrc_lexicon", sentiment)) %>% 
  spread(key = sentiment, n, fill = 0)

```


## **Combining "bing", "afinn" and "nrc" sentiment**

```{r all}

sentiment_all <- bing_sentiment %>% 
  left_join(afinn_sentiment) %>% 
  left_join(nrc_sentiment)
sentiment_all

```

