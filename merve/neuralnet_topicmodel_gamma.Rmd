---
title: "Neural Nets with Topic Modelling Gamma Values"
author: "Merve AYDIN CHESTER"
date: "9/14/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#Data PRep
```{r}
txt_files <- list.files("../sona-text-1994-2018/")

sona <- data.frame(filename = as.character(), speech = as.character())
for(i in txt_files){
  file_name <- paste0("../sona-text-1994-2018/", i)
  
  # import text as single character string (can also read.table but the "seperator" causes problems)
  this_speech <- readChar(file_name, 
                          nchars = file.info(file_name)$size)
  
  # make data frame with metadata (filename contains year and pres) and speech
  this_sona <- data.frame(filename = i, speech = this_speech, stringsAsFactors = FALSE)
  
  # make a single dataset
  sona <- rbind(sona, this_sona)
}

# extract year
sona$year <- str_sub(sona$filename, start = 1, end = 4)

# extract president name
sona$president <- unlist(str_extract_all(sona$filename, '(?<=_)[^_]+(?=.txt)'))
# this finds everything between "_" and ".txt"
```

### Topic Modelling
```{r}
#Stop words are not taken out because the original model also has each and every word, so it would not be 

# data(stop_words)
# sona_tdf <- sona_tdf %>% anti_join(stop_words,by=c("word"="word"))

# Document term maatrix cast_dtm()
dtm_sona <- sona_tdf %>% 
  cast_dtm(ID, word, n)

sona_lda <- LDA(dtm_sona, k = 6, control = list(seed = 1234))
sona_lda

reviews_gamma <- df %>% 
    left_join(tidy(sona_lda, matrix = "gamma") %>% 
    mutate(ID = as.numeric(document)) %>% # some cleaning to make key variable (reviewId) usable
    select(-document) %>%
    spread(key = topic, value = gamma, sep = "_"))

sona_topic_model = tidy(sona_lda, matrix = "gamma") %>% 
    mutate(ID = as.character(document)) %>% # some cleaning to make key variable (reviewId) usable
    select(-document)
sona_gamma <- df %>% 
    left_join(sona_topic_model) %>%
    spread(key = topic, value = gamma, sep = "_") %>% select(-filename, -year, -text, -topic_NA)
```

```{r}
set.seed(321)
training_ids <- sona_gamma %>% 
  group_by(presidents) %>% 
  sample_frac(0.9) %>% 
  ungroup() %>%
  select(ID)

training_sona <- sona_gamma %>% 
  right_join(training_ids, by = "ID") %>%
  select(-ID)

test_sona <- sona_gamma %>% 
  anti_join(training_ids, by = "ID") %>%
  select(-ID)
```

```{r}
# Seperate response variable
dim(sona_gamma) #6626 rows 10626 cols
x_train <- as.matrix(training_sona %>% select(-presidents))
x_test <- as.matrix(test_sona %>% select(-presidents), ncol = 7364)

#One-hot encoding for response variable
labels_test = test_sona %>% select(presidents) %>% unlist()
president_count = labels_test %>% as_tibble() %>% unique() %>% count()
response_tokenizer = text_tokenizer(num_words = president_count + 1)
response_tokenizer$fit_on_texts(labels_test)
# Extract response vector, ignoring first (empty) column
y_test = (response_tokenizer$texts_to_matrix(labels_test, mode = "binary"))[,-1]

labels_train = training_sona %>% select(presidents) %>% unlist()
president_count = labels_train %>% as_tibble() %>% unique() %>% count()
response_tokenizer = text_tokenizer(num_words = president_count + 1)
response_tokenizer$fit_on_texts(labels_train)
# Extract response vector, ignoring first (empty) column
y_train = (response_tokenizer$texts_to_matrix(labels_train, mode = "binary"))[,-1]
```

```{r}

model <- keras_model_sequential() #Creating an empty sequential model

#Define a model by sequentially adding layers.

model %>%
  layer_dense(units = 16, activation = 'relu', input_shape = ncol(x_train)) %>%
  layer_dense(units = 16, activation = 'relu', kernel_regularizer = regularizer_l1_l2(l1 = 0.01, l2 = 0.01), bias_initializer = initializer_glorot_normal()) %>%
  layer_dense(units = 6, activation = 'softmax') %>%
  compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizer_rmsprop(lr = 0.003),
    metrics = c('accuracy')
  )

model %>% fit(x_train, y_train, epochs = 10, batch_size = 32 ) %>% plot()

model %>% evaluate(x_test, y_test)

```
