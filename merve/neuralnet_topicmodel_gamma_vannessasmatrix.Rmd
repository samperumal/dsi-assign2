---
title: "Neural Nets with Topic Modelling Gamma Values"
author: "Merve AYDIN CHESTER"
date: "9/14/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
sona_gamma = topic_matrix
sona_gamma = sona_gamma %>% select(-text, -filename, -year)

set.seed(321)
training_ids <- sona_gamma %>% 
  group_by(president) %>% 
  sample_frac(0.9) %>% 
  ungroup() %>%
  select(sentence_id)

training_sona <- sona_gamma %>% 
  right_join(training_ids, by = "sentence_id") %>%
  select(-sentence_id)

test_sona <- sona_gamma %>% 
  anti_join(training_ids, by = "sentence_id") %>%
  select(-sentence_id)
```

```{r}

# Seperate response variable
dim(sona_gamma) #6626 rows 10626 cols
x_train <- as.matrix(training_sona %>% select(-president))
x_test <- as.matrix(test_sona %>% select(-president), ncol = 7364)

#One-hot encoding for response variable
labels_test = test_sona %>% select(president) %>% unlist()
president_count = labels_test %>% as_tibble() %>% unique() %>% count()
response_tokenizer = text_tokenizer(num_words = president_count + 1)
response_tokenizer$fit_on_texts(labels_test)
# Extract response vector, ignoring first (empty) column
y_test = (response_tokenizer$texts_to_matrix(labels_test, mode = "binary"))[,-1]

labels_train = training_sona %>% select(president) %>% unlist()
president_count = labels_train %>% as_tibble() %>% unique() %>% count()
response_tokenizer = text_tokenizer(num_words = president_count + 1)
response_tokenizer$fit_on_texts(labels_train)
# Extract response vector, ignoring first (empty) column
y_train = (response_tokenizer$texts_to_matrix(labels_train, mode = "binary"))[,-1]
```

```{r}

model <- keras_model_sequential() #Creating an empty sequential model

#Define a model by sequentially adding layers.

model %>%
  layer_dense(units = 16, activation = 'relu', input_shape = ncol(x_train)) %>%
  layer_dense(units = 16, activation = 'relu', kernel_regularizer = regularizer_l1_l2(l1 = 0.01, l2 = 0.01),    bias_initializer = initializer_glorot_normal()) %>%
  layer_dense(units = 6, activation = 'softmax') %>%
  compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizer_rmsprop(lr = 0.003),
    metrics = c('accuracy')
  )

model %>% fit(x_train, y_train, epochs = 10, batch_size = 32 ) %>% plot()

model %>% evaluate(x_test, y_test)

```
