---
title: "Untitled"
author: "Merveaydin"
date: "9/4/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(stringr)
library(lubridate)
library(tidytext)
library(rpart) 
library(wordcloud)
library(RColorBrewer)
options(repr.plot.width=4, repr.plot.height=3) # set plot size in the notebook

```

```{r}
txt_files <- list.files("~/Downloads/sona-text-1994-2018/")

sona <- data.frame(filename = as.character(), speech = as.character())
for(i in txt_files){
  file_name <- paste0("~/Downloads/sona-text-1994-2018/", i)
  
  # import text as single character string (can also read.table but the "seperator" causes problems)
  this_speech <- readChar(file_name, 
                          nchars = file.info(file_name)$size)
  
  # make data frame with metadata (filename contains year and pres) and speech
  this_sona <- data.frame(filename = i, speech = this_speech, stringsAsFactors = FALSE)
  
  # make a single dataset
  sona <- rbind(sona, this_sona)
}

# extract year, testing
str_extract(sona$filename, "[0-9]")
str_extract_all(sona$filename, "[0-90-90-90-9]")
str_extract_all(sona$filename, "[0-9][0-9][0-9][0-9]")
str_extract_all(sona$filename, "[0-9][0-9][0-9][0-9]", simplify = T)
str_extract(sona$filename, "[0-9]{4}")

# does the same thing
str_sub(sona$filename, start = 1, end = 4)

sona$year <- str_sub(sona$filename, start = 1, end = 4)


# unnest_tokens is very useful, can split into words, sentences, lines, paragraphs, etc

# we want to predict sentences, so we need to first split into sentences
sona = sona %>% unnest_tokens(text, speech, token = "sentences")
sona$presidents = gsub(".*[_]([^.]+)[.].*", "\\1", (sona$filename))

# exercise: add an ID variable for sentences and tokenize each sentence by words

df = tibble::rowid_to_column(sona, "ID")
```

```{r}
# df$presidents = factor(df$presidents)
# levels(df$presidents) <- 1:length(levels(df$presidents))
# df$presidents = as.integer(df$presidents)

# exercise: count how many times each word was used in each sentence
sentence_word_count = df %>%
  unnest_tokens(word, text) %>%
  count(ID, word, sort = TRUE)

#Join to get the file name (response)
sentence_word_count = left_join(sentence_word_count, df)
# colnames(sentence_word_count[4]) <- "response"


# exercise: count how many times each word was used in each president's sum of speaches
president_word_count = df %>%
  group_by(presidents) %>%
  unnest_tokens(word, text) %>%
  count(ID, word, sort = TRUE)

#Join to get the file name and presidents
president_word_count = left_join(president_word_count, df)


#Removing the stop words
#data(stop_words)
#tidy_sona <- sentence_word_count %>% anti_join(stop_words,by=c("text"="word"))

 
# exercise: reshape long to wide to get into usual format for predictive models 
#Turning long format into wide
# words_wide <- sentence_word_count %>% spread(key = "word", value = "n")

```

```{r}
#Bind tf_idf
president_word_count <- president_word_count %>%
  bind_tf_idf(word, presidents, n)  

president_word_count %>% arrange(desc(tf_idf))

freq_data = president_word_count %>% group_by(presidents) %>% filter(tf_idf>0) %>% ungroup() %>% arrange(desc(tf_idf))

mean(president_word_count$tf_idf)
```


```{r}

# Calculate distinct number of words in corpus
max_words = freq_data %>% select(word) %>% unique() %>% count()
# Calculate maximum word count in any sentence
max_length= sentence_word_count %>% group_by(ID) %>% summarise(n = n())
min_length = 10
max_length = 150

# # Filter out short sentences
# sona_sentences = sona %>% unnest_tokens(sentence, text, token = "sentences")
# # sentence_data = sona_sentences %>% filter(stringi::stri_length(sentence) > min_length)


# Extract sentences and presidents
sentences = freq_data %>% select(text) %>% unlist()
presidents = freq_data %>% select(presidents) %>% unlist()

# Convert sentences to matrix of input vectors
tokenizer = keras::text_tokenizer(num_words = max_words)
tokenizer$fit_on_texts(sentences)
sequences = tokenizer$texts_to_sequences(sentences)
x_data = pad_sequences(sequences, max_length, padding = "post", truncating = "post")

# One-hot encode president
president_count = presidents %>% as_tibble() %>% unique() %>% count()
response_tokenizer = text_tokenizer(num_words = president_count + 1)
response_tokenizer$fit_on_texts(presidents)
# Extract response vector, ignoring first (empty) column
y_data = (response_tokenizer$texts_to_matrix(presidents, mode = "binary"))[,-1]

# Create test and train sets
train_indices = sample(1:nrow(x_data), 0.9 * nrow(x_data), replace = FALSE)
x_train = (x_data[train_indices,])
x_valid = (x_data[-train_indices,])
y_train = y_data[train_indices,]
y_valid = y_data[-train_indices,]


# Build model
max_features <- max_words  # choose max_features most popular words
embedding_dims <- 80       # number of dimensions for word embedding

model = keras::keras_model_sequential()

model %>%
  # embedding layer maps vocab indices into embedding_dims dimensions
  layer_embedding(max_features, embedding_dims, input_length = max_length) %>%
  # add some dropout
  layer_dropout(0.3) %>%
  # convolutional layer
  layer_conv_1d(
    filters = 64,
    kernel_size = 5,
    padding = "valid",  # "valid" means no padding, as we did it already
    activation = "relu",
    strides = 2
  ) %>%
  layer_global_max_pooling_1d(4) %>%
  layer_dense(1024) %>%
  layer_dropout(0.1) %>%
  layer_activation("relu") %>%
  layer_dense(528) %>%
  layer_dropout(0.3) %>%
  layer_activation("relu") %>%
  layer_dense(256) %>%
  layer_dropout(0.1) %>%
  layer_activation("relu") %>%
  layer_dense(128) %>%
  layer_dropout(0.1) %>%
  layer_activation("relu") %>%
  layer_dense(president_count) %>%  
  # single unit output layer
  layer_activation("softmax")

model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(lr = 0.003, decay = 3e-2),
  metrics = "accuracy"
)

model %>%
  fit(
    x_train, y_train,
    batch_size = 10,
    epochs = 10,
    validation_data = list(x_valid, y_valid)
  )
# dim(x_train) # 6577  150
# dim(y_train) # 6577  6
# dim(x_valid) # 737 150
# dim(y_valid) # 7599 6
# 
# dim(y_data)
# dim(x_data)
```



