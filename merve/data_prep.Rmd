---
title: "Untitled"
author: "Merveaydin"
date: "9/4/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(stringr)
library(lubridate)
library(tidytext)
library(rpart) 
library(wordcloud)
library(RColorBrewer)
options(repr.plot.width=4, repr.plot.height=3) # set plot size in the notebook

```

```{r}
txt_files <- list.files("~/Downloads/sona-text-1994-2018/")

sona <- data.frame(filename = as.character(), speech = as.character())
for(i in txt_files){
  file_name <- paste0("~/Downloads/sona-text-1994-2018/", i)
  
  # import text as single character string (can also read.table but the "seperator" causes problems)
  this_speech <- readChar(file_name, 
                          nchars = file.info(file_name)$size)
  
  # make data frame with metadata (filename contains year and pres) and speech
  this_sona <- data.frame(filename = i, speech = this_speech, stringsAsFactors = FALSE)
  
  # make a single dataset
  sona <- rbind(sona, this_sona)
}

# extract year, testing
str_extract(sona$filename, "[0-9]")
str_extract_all(sona$filename, "[0-90-90-90-9]")
str_extract_all(sona$filename, "[0-9][0-9][0-9][0-9]")
str_extract_all(sona$filename, "[0-9][0-9][0-9][0-9]", simplify = T)
str_extract(sona$filename, "[0-9]{4}")

# does the same thing
str_sub(sona$filename, start = 1, end = 4)

sona$year <- str_sub(sona$filename, start = 1, end = 4)

# exercise: extract president name


# pre-processing data to get into right format for neural nets

library(tidytext)

# unnest_tokens is very useful, can split into words, sentences, lines, paragraphs, etc

# we want to predict sentences, so we need to first split into sentences
sona = sona %>% unnest_tokens(text, speech, token = "sentences")

# exercise: add an ID variable for sentences and tokenize each sentence by words

df = tibble::rowid_to_column(sona, "ID")
colnames(df)[1] <- "sentence_id"

# exercise: count how many times each word was used in each sentence

sentence_word_count = df %>%
  unnest_tokens(word, text) %>%
  count(sentence_id, word, sort = FALSE) %>%
ungroup() %>% 
  arrange(desc(n))


#Join to get the file name (response)
sentence_word_count = left_join(sentence_word_count, df)
sentence_word_count$filename = gsub(".*[_]([^.]+)[.].*", "\\1", sentence_word_count$filename)


#Removing the stop words
#data(stop_words)
#tidy_sona <- sentence_word_count %>% anti_join(stop_words,by=c("text"="word"))

#Taking out regular expressions
# unnest_reg <- "[^A-Za-z\\d_#@']"
# tidy_sona = sentence_word_count %>%  unnest_tokens(word, text, token = "regex", pattern = unnest_reg)
 
# exercise: reshape long to wide to get into usual format for predictive models 
#Turning long format into wide
# words_wide <- sentence_word_count %>% spread(key = "word", value = "n")


```

```{r}
#Bind tf_idf
tidy_sona <- sentence_word_count %>%
  bind_tf_idf(word, filename, n)
tidy_sona

tidy_sona %>%
  select(-year) %>%
  arrange(desc(tf_idf))

table(sentence_word_count$filename)


```

```{r}
(min_class_size <- min(table(sentence_word_count$filename)))
```

```{r}
sentence_word_count_new <- sentence_word_count %>% group_by(filename) %>% sample_n(min_class_size) %>% ungroup()
(table(sentence_word_count_new$filename))
```

```{r}
set.seed(321)
training_ids <- sentence_word_count_new %>% 
  group_by(filename) %>% 
  sample_frac(0.7) %>% 
  ungroup() %>%
  select(sentence_id)

training_sentences <- sentence_word_count_new %>% 
  right_join(training_ids, by = "sentence_id") %>%

test_sentences <- sentence_word_count_new %>% 
  anti_join(training_ids, by = "sentence_id") %>%
```

```{r}
fit <- rpart(factor(sentence_word_count$filename) ~ ., training_sentences)
```



