---
title: "DSI Assignment 2 Final Report"
authors: "Merve Aydin Chester AYDMER001",
         "Audrey Pentz, PNTAUD001",
         "Sam Perumal PRMSAM001",
         "Vanessa Saker, SKRVAN001"
date: "September 20, 2018"
output: html_document
---


```{r setup, include=FALSE}

##---------------------------------##
## ADD ANY LIBRARIES USED HERE
##---------------------------------##

knitr::opts_chunk$set(
  echo = TRUE,
  library(tidyverse),
  library(tidytext),
  library(stringr),
  library(lubridate),
  library(knitr),
  library(wordcloud),
  library(wordcloud2),    # wordcloud
  library(kableExtra),    # tables
  library(formattable),    # coloured cell in a table
  library(keras),
  library(psych)
  
)

##---------------------------------##
## DATA LOADED SO NO NEED TO RELOAD
##---------------------------------##

load("~/Downloads/dsi-assign2/input_data.RData")
load("~/Downloads/dsi-assign2/sentence_data.RData")
load("~/Downloads/dsi-assign2/balanced_train_data.RData")
load("~/Downloads/dsi-assign2/audrey/sentiment_all.Rdata")
load("~/Downloads/dsi-assign2/vanessa/topic_matrix.RData")
```


#### **Description of the Problem**

We were given 30 State of the Nation (SONA) speeches from 1994 to 2018 to analyse. The specific objectives are to: 
1. Infer sentiment and changes in sentiment over time  
2. Describe the topics that emerge  
3. Predict the President from a given sentence of text  
4. Evaluate out of sample performance of the predictions  


#### **Approach**  

We collaborated using the following GitHub location: https://github.com/samperumal/dsi-assign2. 

We initially split the work as follows and each of us created a folder with our names to push our work to for others to view:  
- Neural Net - Sam and Merve
- Bag of Words - Merve
- Topic Modelling - Vanessa
- Sentiment Analysis - Audrey

We presented our work to each other and made suggestions for improvement. The initial results from the Neural Net gave a 65% accuracy on the validation set and we wanted to feed the results of the Topic Modelling and Sentiment Analysis into the Neural Net to see if it would improve results so we needed to understand from each other what the output of these 2 methods was and the input required by the neural net to get the data into a useable format which took some discussion and a few iterations.

Given the low accuracy of the neural net (NN), we also tried a Convolutional Neural Net (CNN). Sam got the initial model working. Merve made improvements. Vanessa tuned the hyperparamters.
...need more here...

The cnn did not provide much improvement over the NN so we also tried a Recurrent Neural Net (RNN) which takes in sequences of data so the order of the words is also taken into account in the model.
...need more here...

Initially we each performed our own import of the data, splitting out the year and president and tokenisation but we realised there was duplication of effort here and different naming conventions which made it difficult to collaborate and use each other's output. In addition, Sam noticed that some of the data was not being read in because of special characters and sentences were not being tokenised correctly for various reasons so he became responsible for performing the data clean up (preprocessing) and outputting a .RData file that we could all then use to rerun our work. This is explained in more detail below and at a high level includes:   
- removing special characters such as slashes, currency symbols, currency values and numbers  
- changing bullet points, colons, semi-colons and ellipsis to full stops  
- changing the word after a full stop to sentence case  
- removing multiple spaces and multiple full stops  

Sam also assigned sentence id's using hashing and split the data into training and validation sets so that there would be consistency across what we were working on so that we could use each others work and compare results more easily.

```{r preprocessing}

# sam

```

Before diving into any analysis, we felt it is important to do an Exploratory Data Analysis (EDA) to get a sense of the high level overview of the dataset. This was done by Audrey.

#### **Overview of the dataset**  

Each president has made a certain number of SONA speeches, depending on their term in office and whether there was 1 speech that year or 2 in the year of an election (pre and post election). Since the data is dependent on their term in the office it is unbalanced. Sentence counts per president after cleaning the data is :
```{r}
paste0("President sentence counts:")
(table(input_data$sentences$president))
paste0("Baseline_accuracies")
(baseline_accuracies = table(input_data$sentences$president)*100/sum(table(input_data$sentences$president)))
```

Let's understand the number of words used by each President and how this varies across each SONA speech. 

#### **Average number of words used per President**  

We need to create a metric called "avg_words" which is simply the total number of words across all SONA speeches made by a particular president, divided by the total number of SONA speeches that president made.  

```{r descPres}

# sentence tokenization
tidy_sona_sentences <- input_data$sentences

# word tokenization
tidy_sona_words <- input_data$words %>% 
  filter(!word %in% stop_words$word, str_detect(word, "[a-z]"))  # remove stop words

# bigram tokenization
bigrams <- tidy_sona_sentences %>% 
  unnest_tokens(bigram, sentence, token = "ngrams", n = 2)
# separate the bigrams 
bigrams_separated <- bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")
# remove stop words
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)
# join up the bigrams again
tidy_sona_bigrams <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

# speeches
speech_count <- tidy_sona_words %>%
  group_by(year, president) %>%
  count() %>% 
  group_by(president) %>% 
  summarise(num_speeches = n())

# avg words per president
avg_word_count <- tidy_sona_words %>%
  group_by(president) %>% 
  summarise(num_words = n()) %>% 
  left_join(speech_count) %>% 
  mutate(avg_words = round((num_words/num_speeches),0)) %>% 
  arrange(desc(avg_words))

# plot avg words per president
avg_word_count %>%
  ungroup(avg_words, president) %>%
  mutate(avg_words = color_bar("lightblue")(avg_words)) %>%
  kable("html", escape = FALSE, align = "l", caption = "Average number of words used per President") %>%
  kable_styling(bootstrap_options = 
                  c("striped", "condensed", "bordered"), 
                  full_width = FALSE)

```
  
## **Insights:**   

On average, Mbeki used the most words in his SONA speeches, followed by Motlanthe and de Klerk used the least. Mandela and Zuma are ranked in the middle of their peers. The current president (Ramaphosa) used fewer words than all of his post 1994 peers.  

#### **Number of words used per SONA**  

```{r descSONA}

# per SONA
word_count <- tidy_sona_words %>%
  group_by(president, year, election) %>%
  summarise(num_words = n()) %>%
  arrange(desc(num_words)) 

# plot words per SONA
word_count %>%
  ggplot(aes(x = as.numeric(year), y = num_words, colour = president)) +
  geom_point() + 
  geom_smooth(method = "loess", aes(colour = president))

```
  
## **Insights:**   

Of the 3 presidents that have made more than 1 SONA speech, Mbeki used more words on average than both Mandela and Zuma and the variance in the number of words used per SONA speech is also higher for Mbeki. In 2004, which was an election year, the average number of words Mbeki used was lower in both his pre- and post-election speeches. Towards the end of his term, his average number of words also dropped off. The data suggests that perhaps Mbeki's average number of words is correlated with his confidence in being re-elected President.  



#### **Common words used across all SONA speeches**  

```{r}

# word cloud for words
word_count <- tidy_sona_words %>%
  count(word, sort = TRUE)

wordcloud2(word_count[1:300, ], size = .6, color='random-dark')

```
  
## **Insights:**   

....  


#### **Common bigrams used across all SONA speeches**  

```{r}

# word cloud for bigrams
bigram_count <- tidy_sona_bigrams %>%
  count(bigram, sort = TRUE)

wordcloud2(bigram_count[1:300, ], size = .8, color='random-dark')

```
  
## **Insights:**   

....



#### **Lexical Diversity per President**  

Lexical diversity refers to the number of unique words used in each SONA.

```{r diversity}

# word tokenization
diversity_per_year <- input_data$words %>%   # don't remove stop words this time
  group_by(president, year) %>% 
  summarise(diversity = n_distinct(word)) %>% 
  arrange(desc(diversity))

diversity_per_year %>%
  ggplot(aes(x = as.numeric(year), y = diversity, colour = president)) +
  geom_point(color = "steelblue",
               alpha = .7,               # transparency
               size = 3,                 # point size
               position = "jitter") +    # point overlap
  geom_smooth(method = "loess", aes(colour = president)) +
  ggtitle("Lexical Diversity per President") +
  xlab("year") +
  ylab("")

```
  
## **Insights:**   

The number of unique words per SONA ranges from about 700 with de Klerk in 1994 to over 2500 with Mandela in his post election speech of 1999. Mbeki's post election speech of 2004 and Zuma's post election speech of 2014 also got close to the 2500 mark.  

It's interesting that whilst the trend in the number of unique words used was most often upwards with Mandela, Mbeki and Zuma both show a mostly upward trend in the lead up to the election year, followed by a mostly downward trend after nearing the 2500 unique words mark in their post election speech.  

If we exclude the post election speeches, the number of unique words used by Mbeki during his term from 2000 to 2008 averages just under 2000 whereas the number of unique words used by Zuma during his term from 2009 to 2017 averages just over 1500.  



#### **Lexical Density per President**  

Lexical density refers to the number of unique words used in each SONA divided by the total number of words and a high value is an indicator of word repitition.  

```{r density}

# word tokenization
density_per_year <- input_data$words %>%   # don't remove stop words this time
  group_by(president, year) %>% 
  summarise(density = n_distinct(word) / n()) %>% 
  arrange(desc(density))

density_per_year %>%
  ggplot(aes(x = as.numeric(year), y = density, colour = president)) +
  geom_point(color = "steelblue",
               alpha = .7,               # transparency
               size = 3,                 # point size
               position = "jitter") +    # point overlap
  geom_smooth(method = "loess", aes(colour = president)) +
  ggtitle("Lexical Density per President") +
  xlab("year") +
  ylab("")

```
  
## **Insights:**   

De Klerk repeated over 30% of his words in his 1994 pre election SONA speech. On average, Mandela repeated about 25% of words in each of his SONA speeches and this reduced to about 20% in the post election speech of 1999. Mbeki's repitition rate was about 23% and this reduced to 20% in the post election speech of 2004. Zuma's repitition rate is over 30% with the exception of his post election speech of 2014 at about 23%.  



# **Results**




#### **Sentiment Analysis**

```{r sentiment}

# audrey

```
## **Topic Modelling**

```{r topic}

# vanessa

```
#### **Neural Nets **

### **Neural Net with Bag of Words Data**

The count of each word that have been used in each sentence is what we are going to be feeding in. We need to unnest the sentence data, count each word in each sentence and spread the sentence word counts so that we have sentence id's in each row and we have each word as column id's. This is the simplest neural net model that we can try, so that was our first model.

```{r}
#Unnest words and count
president_word_count = input_data$sentences %>%
  group_by(president) %>%
  unnest_tokens(word, sentence) %>%
  count(id, word, sort = TRUE) %>%
  ungroup()
  
#Spread the words
bag_of_words <- president_word_count %>% 
  select(ID = id, prez = president,  word,n) %>% 
  spread(key = word, value = n, fill = 0)

#Set training and testing sets
set.seed(321)
training_ids <- bag_of_words %>% 
  group_by(prez) %>% 
  sample_frac(0.9) %>% 
  ungroup() %>%
  select(ID)

training_sona <- bag_of_words %>% 
  right_join(training_ids, by = "ID") %>%
  select(-ID)

test_sona <- bag_of_words %>% 
  anti_join(training_ids, by = "ID") %>%
  select(-ID)
nrow(bag_of_words)

# Seperate response variable
dim(training_sona) #6626 rows 10626 cols
x_train <- as.matrix(training_sona %>% select(-prez))
x_test <- as.matrix(test_sona %>% select(-prez), ncol = 10625)

#One-hot encoding for response variable
labels_test = test_sona %>% select(prez) %>% unlist()
president_count = labels_test %>% as_tibble() %>% unique() %>% count()
response_tokenizer = text_tokenizer(num_words = president_count + 1)
response_tokenizer$fit_on_texts(labels_test)
# Extract response vector, ignoring first (empty) column
y_test = (response_tokenizer$texts_to_matrix(labels_test, mode = "binary"))[,-1]

labels_train = training_sona %>% select(prez) %>% unlist()
president_count = labels_train %>% as_tibble() %>% unique() %>% count()
response_tokenizer = text_tokenizer(num_words = president_count + 1)
response_tokenizer$fit_on_texts(labels_train)
# Extract response vector, ignoring first (empty) column
y_train = (response_tokenizer$texts_to_matrix(labels_train, mode = "binary"))[,-1]

#Create a model, define a model by sequentially adding layers.
model <- keras_model_sequential() #Creating an empty sequential model
model %>%
  layer_dense(units = 64, activation = 'relu', input_shape = ncol(x_train)) %>%
  layer_dense(units = 64, activation = 'relu', kernel_regularizer = regularizer_l2(0.01)) %>%
  layer_dense(units = 6, activation = 'softmax') %>%
  compile(
  optimizer = optimizer_rmsprop(lr = 0.003),
  loss = 'categorical_crossentropy',
  metrics = c('accuracy')
)
#See the number of parameteres
summary(model)

#Compile and fit the model
model %>% fit(x_train, y_train, epochs = 10, batch_size = 32, validation_data = list(x_test, y_test)
) %>% plot()

#Evaluate the model
(test_accuracy_wordcount_bigger = model %>% evaluate(x_test, y_test, batch_size=32, verbose = 1)) #0.55886
(train_accuracy_wordcount_bigger = model %>% evaluate(x_train, y_train, batch_size=32, verbose = 1))
```
This model has L2 regularization to avoid overfitting but even so it didn't help very much. The accuracy is 0.55886. The optimizer_rmsprop has learning rate 0.003. This was choosen after trying lr=c(0.001, 0.002, 0.003) To make readability easier the model with best learning rate is used. 

As we can see from the plot model overfits after the second iteration, since the loss function start increasing in value, so to avoid that let's use a smaller model with less neurons and add a dropout.
```{r}
model <- keras_model_sequential() #Creating an empty sequential model

#Define a model by sequentially adding layers. Add a simple model so it does not overfit.

model %>%
  layer_dense(units = 16, activation = 'relu', input_shape = ncol(x_train)) %>%
  layer_dropout(0.03) %>%
  layer_dense(units = 16, activation = 'relu', 
              kernel_regularizer = regularizer_l2(0.01)) %>%  #Dense layer with L2 regularizer 
  layer_dropout(0.01) %>%
  layer_dense(units = 6, activation = 'softmax') %>%       #6 neurons since we have six presidents
  compile(
  optimizer = optimizer_rmsprop(lr = 0.003),  
  loss = 'categorical_crossentropy',
  metrics = c('accuracy')
)

#See the number of parameteres:
summary(model)

model %>% fit(x_train, y_train, epochs = 10, batch_size = 32, validation_data = list(x_test, y_test)
) %>% plot()

model %>% evaluate(x_test, y_test, batch_size=32, verbose = 1) #0.5811648

#Test and Train Accuracies
(test_accuracy_wordcount_smaller = model %>% evaluate(x_test, y_test, batch_size=32, verbose = 1)) #0.5811648
#This smaller model gave a better accuracy result than the one that has no dropout 

(train_accuracy_wordcount_smaller = model %>% evaluate(x_train, y_train, batch_size=32, verbose = 1))

#Get the predictions for the model
predictions <- model %>% predict(x_test)
predictions2 = model %>% predict_classes(x_test)
range(predictions)
range(predictions2)


#Getting the max of each rows prediction and binding it on the end
pres.max <- as.data.frame(predictions)
pres.max <- apply(predictions,1,which.max) 
pres.max <- unlist(as.numeric(as.character(pres.max)))
pres.max <- cbind(predictions,pres.max )

pres.actual <- as.data.frame(y_test)
pres.actual <- apply(y_test,1,which.max) 
pres.actual = as.data.frame(pres.actual, ncol=1)

predictions_val <- cbind(test_sona, pres.actual, pres.max)
predictions_val = predictions_val %>% select(prez, pres.actual, pres.max)


table <- table(predictions_val$pres.actual, predictions_val_2$pres.max)
paste0("Accuracy rate is:" ,round((sum(diag(table))/sum(table)),4))


# Calculate kappa by hand
t = as.matrix(table)
kappa(t)


t = cbind(t, "4" = colSums(t)/sum(colSums(t)))
t = rbind(t, "7" = rowSums(t)/sum(rowSums(t)))
t[7,4] = 0

prob_a = sum(diag(table))/sum(colSums(table))
prob_b = (t[7,1]*t[1,4]) + (t[7,2]*t[2,4]) +(t[7,3]*t[3,4])

k = (prob_a - prob_b)/(1-prob_b)

paste0("Kappa value is ", round(k, 3), ", and this means we are doing better than random")

# Kappa value tells you how much better your classifier is performing over the performance of a classifier that simply guesses at random according to the frequency of each class.
```

The accuracy is slightly better than bigger model with no-dropouts (0.581%)
Just for the word-count model seems good enough. But this model does not consider how important each word is to its corpus. So we should consider a better model to try.

After the fourth iteration validation loss starts increasing which is a sign of overfitting. 

## **Neural Net with tf-idf Data**

TFIDF is a statisic that shows how important a word is to it's corpus. So if we are feeding NN with TFIDF we are logically expecting the results to be slightly better than the word-cout NN model. 

```{r tfidf}
tidy_sona = input_data$sentences %>%
  unnest_tokens(word, sentence, token = "words") %>% 
  select(id, word, year, president) 

sona_tdf <- tidy_sona %>%
  select(id,word) %>%
  group_by(id,word) %>%
  count() %>%  
  group_by(id) %>%
  mutate(total = sum(n)) %>%
  ungroup()

sona_tf_idf <- sona_tdf %>% 
  bind_tf_idf(word, id, n) # replace with values from tidytext

#Get presidents names where id's match
nm <- c("id", "president")
df = input_data$sentences
sona_tf_idf[nm] <- lapply(nm, function(x) df[[x]][match(sona_tf_idf$id, input_data$sentences$id)])


# Spreading the data

tf_idf_wide <- sona_tf_idf %>% 
  select(ID = id, prez = president, word, tf_idf) %>%  # note the change, using tf-idf
  spread(key = word, value = tf_idf, fill = 0)

#Create training anf testing set
set.seed(321)
training_ids <- tf_idf_wide %>% 
  group_by(prez) %>% 
  sample_frac(0.9) %>% 
  ungroup() %>%
  select(ID)

training_sona <- tf_idf_wide %>% 
  right_join(training_ids, by = "ID") %>%
  select(-ID)

test_sona <- tf_idf_wide %>% 
  anti_join(training_ids, by = "ID") %>%
  select(-ID)

# Seperate response variable and normalize the data
dim(training_sona) #6626 rows 10626 cols
x_train <- normalize(as.matrix(training_sona %>% select(-prez))) 
x_test <- normalize(as.matrix(test_sona %>% select(-prez)))

#One-hot encoding for response variable
labels_test = test_sona %>% select(prez) %>% unlist()
president_count = labels_test %>% as_tibble() %>% unique() %>% count()
response_tokenizer = text_tokenizer(num_words = president_count + 1)
response_tokenizer$fit_on_texts(labels_test)
# Extract response vector, ignoring first (empty) column
y_test = (response_tokenizer$texts_to_matrix(labels_test, mode = "binary"))[,-1]

labels_train = training_sona %>% select(prez) %>% unlist()
president_count = labels_train %>% as_tibble() %>% unique() %>% count()
response_tokenizer = text_tokenizer(num_words = president_count + 1)
response_tokenizer$fit_on_texts(labels_train)
# Extract response vector, ignoring first (empty) column
y_train = (response_tokenizer$texts_to_matrix(labels_train, mode = "binary"))[,-1]

#Define the model

model <- keras_model_sequential() #Creating an empty sequential model
model %>%
  layer_dense(units = 8, activation = 'relu', input_shape = ncol(x_train)) %>%
  layer_dense(units = 8, activation = 'relu', kernel_regularizer = regularizer_l2(0.01)) %>%
  layer_dense(units = 6, activation = 'softmax') %>%
  compile(
  optimizer = optimizer_rmsprop(lr=0.003),
  loss = 'categorical_crossentropy',
  metrics = c('accuracy')
)
model %>% fit(x_train, y_train, epochs = 10, batch_size = 32, validation_data = list(x_test, y_test)
) %>% plot()


#Test and Training Accuracies
(train_accuracy_tfidf = model %>% evaluate(x_train, y_train))$acc
(test_accuracy_tfidf = model %>% evaluate(x_test, y_test))$acc #0.6146221 test accuracy

#Get predictions
predictions <- model %>% predict(x_test)
predictions2 = model %>% predict_classes(x_test)
range(predictions)
range(predictions2)


#Getting the max of each rows prediction and binding it on the end
pres.max <- as.data.frame(predictions)
pres.max <- apply(predictions,1,which.max) 
pres.max <- unlist(as.numeric(as.character(pres.max)))
pres.max <- cbind(predictions,pres.max )

pres.actual <- as.data.frame(y_test)
pres.actual <- apply(y_test,1,which.max) 
pres.actual = as.data.frame(pres.actual, ncol=1)

predictions_val <- cbind(test_sona, pres.actual, pres.max)
predictions_val = predictions_val %>% select(prez, pres.actual, pres.max)


table <- table(predictions_val$pres.actual, predictions_val_2$pres.max)
paste0("Accuracy rate is:" , round((sum(diag(t))/sum(t)),4))

# Calculate kappa by hand
t = as.matrix(table)
t = cbind(t, "4" = colSums(t)/sum(colSums(t)))
t = rbind(t, "7" = rowSums(t)/sum(rowSums(t)))
t[7,4] = 0

prob_a = sum(diag(table))/sum(colSums(table))
prob_b = (t[7,1]*t[1,4]) + (t[7,2]*t[2,4]) +(t[7,3]*t[3,4])

k = (prob_a - prob_b)/(1-prob_b)


paste0("Kappa value is ", round(k, 3), ", and this means the results are better than a random classifier")

```
The accuracy is 0.6158 and the model starts overfitting after fourth epoch. So this is slightly better than the bag of words word-count model as we expected.

###**Neural Net with Sentiment Analysis Data**
```{r nnwith}
#The preparation of this file can be found in the same directory as the .Rdata file.
load("~/Downloads/dsi-assign2/audrey/sentiment_all.Rdata")

set.seed(123)

training_ids <- sentiment_all %>% 
  group_by(president) %>% 
  sample_frac(0.9) %>% 
  ungroup() %>%
  select(id)

training_sona <- sentiment_all %>% 
  right_join(training_ids, by = "id") %>%
  select(-id)

test_sona <- sentiment_all %>% 
  anti_join(training_ids, by = "id") %>%
  select(-id)


# Seperate response variable
dim(sentiment_all) #8068   34
x_train <- as.matrix(training_sona %>% select(-president, -year, -election))
x_test <- as.matrix(test_sona %>% select(-president, -year, -election))


#One-hot encoding for response variable
labels_test = test_sona %>% select(president) %>% unlist()
president_count = labels_test %>% as_tibble() %>% unique() %>% count()
response_tokenizer = text_tokenizer(num_words = president_count+1)
response_tokenizer$fit_on_texts(labels_test)
# Extract response vector, ignoring first (empty) column
y_test = (response_tokenizer$texts_to_matrix(labels_test, mode = "binary"))[,-1]

labels_train = training_sona %>% select(president) %>% unlist()
president_count = labels_train %>% as_tibble() %>% unique() %>% count()
response_tokenizer = text_tokenizer(num_words = president_count + 1)
response_tokenizer$fit_on_texts(labels_train)
# Extract response vector, ignoring first (empty) column
y_train = (response_tokenizer$texts_to_matrix(labels_train, mode = "binary"))[,-1]

model <- keras_model_sequential() #Creating an empty sequential model

#Define a model by sequentially adding layers.

model %>%
  layer_dense(units = 16, activation = 'relu', input_shape = ncol(x_train)) %>%
  layer_dense(units = 16, activation = 'relu', kernel_regularizer = regularizer_l1_l2(l1 = 0.01, l2 = 0.01), bias_initializer = initializer_glorot_normal()) %>%
  layer_dense(units = 6, activation = 'softmax') %>%
  compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizer_rmsprop(lr = 0.003),
    metrics = c('accuracy')
  )

model %>% fit(x_train, y_train, epochs = 20, batch_size = 32 ,validation_data = list(x_test, y_test)) %>% plot()

#Test and Training Accuracies
(test_accuracy_sentiment = (model %>% evaluate(x_test, y_test))$acc)
#Training accuracy
(train_accuracy_sentiment = (model %>% evaluate(x_train, y_train))$acc)


paste0("Accuracy of sentiment analysis is ", round(accuracy_sentiment,3))

#Get the predictions
predictions <- model %>% predict(x_test)
predictions2 = model %>% predict_classes(x_test)
range(predictions)
range(predictions2)


#Getting the max of each rows prediction and binding it on the end
pres.max <- as.data.frame(predictions)
pres.max <- apply(predictions,1,which.max) 
pres.max <- unlist(as.numeric(as.character(pres.max)))
pres.max <- cbind(predictions,pres.max )

pres.actual <- as.data.frame(y_test)
pres.actual <- apply(y_test,1,which.max) 
pres.actual = as.data.frame(pres.actual, ncol=1)

predictions_val <- cbind(test_sona, pres.actual, pres.max)
predictions_val = predictions_val %>% select(president, pres.actual, pres.max)


table <- table(predictions_val$pres.actual, predictions_val$pres.max)
paste0("Accuracy rate is:" , round((sum(diag(t))/sum(t)),4))
```
## Cohen's Kappa

'Cohen¢s kappa is always less than or equal to 1. Values of 0 or less, indicate that the classifier is useless. There is no standardized way to interpret its values. Landis and Koch (1977) provide a way to characterize values. According to their scheme a value < 0 is indicating no agreement , 0???0.20 as slight, 0.21???0.40 as fair, 0.41???0.60 as moderate, 0.61???0.80 as substantial, and 0.81???1 as almost perfect agreement.'
[Reference: Landis, J.R.; Koch, G.G. (1977). ???The measurement of observer agreement for categorical data???. Biometrics 33 (1): 159???174]

```{r}
# # Calculate kappa by hand to see if it matches cohen.kappa() result.
# t = as.matrix(table)
# t = cbind(t, "4" = colSums(t)/sum(colSums(t)))
# t = rbind(t, "7" = rowSums(t)/sum(rowSums(t)))
# t[7,4] = 0
# 
# prob_a = sum(diag(table))/sum(colSums(table))
# prob_b = (t[7,1]*t[1,4]) + (t[7,2]*t[2,4]) +(t[7,3]*t[3,4])
# 
# k = (prob_a - prob_b)/(1-prob_b)
# 
# paste0("Kappa value is ", round(k, 4), ", and the classifier is slightly better than random classifier") #0.1285


#As we have seen from the table above the nn did not predict anything for hte minority classes so we have to cbind zeros as 4th,5th,6th columns. Because we need a square matrix for calculating Cohen's Kappa when using cohen.kappa() function.

table = cbind(table, "4" = rep(0, 6), "5" = rep(0, 6),"6" = rep(0, 6))
library(psych)
cohen.kappa(table)

```
Sentiment analysis also reaches it's smallest validation loss value on the fifth iteration. But the train accuracy and test accuracy is changing very slightly at each iteration. This model does not seem to be doing well on eighter the training set or the test set. The test accuracy is 0.4361834 and the training accuracy is 0.4353395. If we look at the NRC sentiment lexicon it is visible that all presidents share same sentiment distribution pattern, so this is why it is not overfitting. Because our set aside test set is actually no different than the training set.

###**Neural Nets with Topic Modelling Data (Gamma Values)**
```{r}
#The preparation of this topic_matrix can be found in the 
load("~/Downloads/dsi-assign2/vanessa/topic_matrix.RData")

sona_gamma = topic_matrix
sona_gamma[is.na(sona_gamma)] <- 0
sona_gamma = sona_gamma %>% select(-year, -election, -sentence)

#Prepare train-test indices
set.seed(321)
training_ids <- sona_gamma %>% 
  group_by(president) %>% 
  sample_frac(0.9) %>% 
  ungroup() %>%
  select(id)

training_sona <- sona_gamma %>% 
  right_join(training_ids, by = "id") %>%
  select(-id)

test_sona <- sona_gamma %>% 
  anti_join(training_ids, by = "id") %>%
  select(-id)


# Seperate response variable
dim(sona_gamma) #6626 rows 10626 cols
x_train <- as.matrix(training_sona %>% select(-president))
x_test <- as.matrix(test_sona %>% select(-president), ncol = 7364)

#One-hot encoding for response variable
labels_test = test_sona %>% select(president) %>% unlist()
president_count = labels_test %>% as_tibble() %>% unique() %>% count()
response_tokenizer = text_tokenizer(num_words = president_count + 1)
response_tokenizer$fit_on_texts(labels_test)
# Extract response vector, ignoring first (empty) column
y_test = (response_tokenizer$texts_to_matrix(labels_test, mode = "binary"))[,-1]

labels_train = training_sona %>% select(president) %>% unlist()
president_count = labels_train %>% as_tibble() %>% unique() %>% count()
response_tokenizer = text_tokenizer(num_words = president_count + 1)
response_tokenizer$fit_on_texts(labels_train)
# Extract response vector, ignoring first (empty) column
y_train = (response_tokenizer$texts_to_matrix(labels_train, mode = "binary"))[,-1]


#Create the model
model <- keras_model_sequential() #Creating an empty sequential model

#Define a model by sequentially adding layers.

model %>%
  layer_dense(units = 16, activation = 'relu', input_shape = ncol(x_train)) %>%
  layer_dense(units = 16, activation = 'relu', kernel_regularizer = regularizer_l1_l2(l1 = 0.01, l2 = 0.01),    bias_initializer = initializer_glorot_normal()) %>%
  layer_dense(units = 6, activation = 'softmax') %>%
  compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizer_rmsprop(lr = 0.003),
    metrics = c('accuracy')
  )

model %>% fit(x_train, y_train, epochs = 10, batch_size = 32, validation_data = list(x_test, y_test)) %>% plot()

#Test and Training Accuracies
(train_accuracy_topic = model %>% evaluate(x_train, y_train))$acc
(test_accuracy_topic = model %>% evaluate(x_test, y_test))$acc


#Get predictions
predictions <- model %>% predict(x_test)
predictions2 = model %>% predict_classes(x_test)
range(predictions)
range(predictions2)


#Getting the max of each rows prediction and binding it on the end
pres.max <- as.data.frame(predictions)
pres.max <- apply(predictions,1,which.max) 
pres.max <- unlist(as.numeric(as.character(pres.max)))
pres.max <- cbind(predictions,pres.max )

pres.actual <- as.data.frame(y_test)
pres.actual <- apply(y_test,1,which.max) 
pres.actual = as.data.frame(pres.actual, ncol=1)

predictions_val <- cbind(test_sona, pres.actual, pres.max)
predictions_val = predictions_val %>% select(prez, pres.actual, pres.max)


table <- table(predictions_val$pres.actual, predictions_val_2$pres.max)
paste0("Accuracy rate is:" , round((sum(diag(t))/sum(t)),4))

# Calculate kappa by hand
t = as.matrix(table)
t = cbind(t, "4" = colSums(t)/sum(colSums(t)))
t = rbind(t, "7" = rowSums(t)/sum(rowSums(t)))
t[7,4] = 0

prob_a = sum(diag(table))/sum(colSums(table))
prob_b = (t[7,1]*t[1,4]) + (t[7,2]*t[2,4]) +(t[7,3]*t[3,4])

k = (prob_a - prob_b)/(1-prob_b)

paste0("Kappa value is ", round(k, 3), ", and this means the results have no agreement!")
```
The train and the test set are not very distinct from each other just like sentiment analysis. 
If we look at the mixture of the topics by each president in topic modelling chunk we can see that all the topics for each president are kind of uniformed and hard to seperate each president's topic from one another.

###**CNN with max-pooling Using Transfer Learning (Pre-trained Embeddings)**

We will be using GloVe embeddings. GloVe stands for "Global Vectors for Word Representation" and is an unsupervised learning algorithm for obtaining vector representations for words as it is stated in their website. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.
[Reference:Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation:https://nlp.stanford.edu/pubs/glove.pdf ]
Specifically, we will use the 100-dimensional GloVe embeddings of 400k words computed on a 2014 dump of English Wikipedia. 

This note is taken from the reference given above and as they state the accuracy they achieved on python is twice as good.

"IMPORTANT NOTE:This example does yet work correctly. The code executes fine and appears to mimic the Python code upon which it is based however it achieves only half the training accuracy that the Python code does so there is clearly a subtle difference. We need to investigate this further before formally adding to the list of examples"

[reference for implementation on Python: https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html]
[reference for implementation on R: https://keras.rstudio.com/articles/examples/pretrained_word_embeddings.html]
[reference for implementation on R: https://github.com/rstudio/keras/blob/master/vignettes/examples/pretrained_word_embeddings.R]


Also for the pre-trained embeddings to work well it needs to be trained with similar type of data that you are trying to classify, and the fact that the GloVe embeddings are trained on Wikipedia data one can expect that it would not neccesarily help predict the presidents better for our sentences.

```{r}
library(keras)

#Load the data we are going to be embedding
load("~/Downloads/dsi-assign2/input_data.RData")
load("~/Downloads/dsi-assign2/balanced_train_data.RData")
load("~/Downloads/dsi-assign2/sentence_data.RData")

#Settings for gloves embedding
GLOVE_DIR <- 'glove.6B'
MAX_SEQUENCE_LENGTH <- 1000
MAX_NUM_WORDS <- 40000
EMBEDDING_DIM <- 100
VALIDATION_SPLIT <- 0.2

# download data if necessary
download_data <- function(data_dir, url_path, data_file) {
  if (!dir.exists(data_dir)) {
    download.file(paste0(url_path, data_file), data_file, mode = "wb")
    if (tools::file_ext(data_file) == "zip")
      unzip(data_file, exdir = tools::file_path_sans_ext(data_file))
    else
      untar(data_file)
    unlink(data_file)
  }
}
download_data(GLOVE_DIR, 'http://nlp.stanford.edu/data/', 'glove.6B.zip')

# first, build index mapping words in the embeddings set
# to their embedding vector

cat('Indexing word vectors.\n')

embeddings_index <- new.env(parent = emptyenv())
lines <- readLines(file.path(GLOVE_DIR, 'glove.6B.100d.txt'))
for (line in lines) {
  values <- strsplit(line, ' ', fixed = TRUE)[[1]]
  word <- values[[1]]
  coefs <- as.numeric(values[-1])
  embeddings_index[[word]] <- coefs
}

cat(sprintf('Found %s word vectors.\n', length(embeddings_index)))

# second, prepare text samples and their labels
cat('Tokenizing sona sentences\n')


# finally, vectorize the text samples into a 2D integer tensor
tokenizer <- text_tokenizer(num_words=MAX_NUM_WORDS)
tokenizer %>% fit_text_tokenizer(input_data$sentences$sentence)

# save the tokenizer in case we want to use it again
# for prediction within another R session, see:
# https://keras.rstudio.com/reference/save_text_tokenizer.html
save_text_tokenizer(tokenizer, "tokenizer")

sequences_train = texts_to_sequences(tokenizer, balanced_train_data$sentence)
sequences_valid = texts_to_sequences(tokenizer, sentence_data$validate$sentence)

word_index <- tokenizer$word_index
cat(sprintf('Found %s unique tokens.\n', length(word_index)))

x_train <- pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)
x_val <- pad_sequences(sequences_valid, maxlen=MAX_SEQUENCE_LENGTH)

# Fit president tokenizer
president_count = balanced_train_data$president %>% as_tibble() %>% unique() %>% count()
response_tokenizer = text_tokenizer(num_words = president_count + 1)
response_tokenizer$fit_on_texts(balanced_train_data$president)
response_tokenizer$fit_on_texts(sentence_data$validate$president )

# One-hot encode president
# Extract response vector, ignoring first (empty) column
y_train = (response_tokenizer$texts_to_matrix(balanced_train_data$president, mode = "binary"))[,-1]
y_val = (response_tokenizer$texts_to_matrix(sentence_data$validate$president , mode = "binary"))[,-1]

cat('Shape of data tensor: ', dim(x_train), '\n')
cat('Shape of data tensor: ', dim(x_val), '\n')
cat('Shape of label tensor: ', dim(y_train), '\n')
cat('Shape of data tensor: ', dim(y_val), '\n')


# prepare embedding matrix
num_words <- min(MAX_NUM_WORDS, length(word_index) + 1)
prepare_embedding_matrix <- function() {
  embedding_matrix <- matrix(0L, nrow = num_words, ncol = EMBEDDING_DIM)
  for (word in names(word_index)) {
    index <- word_index[[word]]
    if (index >= MAX_NUM_WORDS)
      next
    embedding_vector <- embeddings_index[[word]]
    if (!is.null(embedding_vector)) {
      # words not found in embedding index will be all-zeros.
      embedding_matrix[index,] <- embedding_vector
    }
  }
  embedding_matrix
}

embedding_matrix <- prepare_embedding_matrix()

# load pre-trained word embeddings into an Embedding layer
# note that we set trainable = False so as to keep the embeddings fixed
embedding_layer <- layer_embedding(
  input_dim = num_words,
  output_dim = EMBEDDING_DIM,
  weights = list(embedding_matrix),
  input_length = MAX_SEQUENCE_LENGTH,
  trainable = FALSE
)

cat('Training model\n')

# train a 1D convnet with global maxpooling
sequence_input <- layer_input(shape = list(MAX_SEQUENCE_LENGTH), dtype='int32')

preds <- sequence_input %>%
  embedding_layer %>% 
  layer_conv_1d(filters = 64, kernel_size = 5, activation = 'relu',
                use_bias = TRUE, kernel_initializer = "glorot_uniform") %>% 
  layer_dropout(0.2) %>%
  layer_max_pooling_1d(pool_size = 5) %>% 
  layer_conv_1d(filters = 64, kernel_size = 5, activation = 'relu') %>% 
  layer_max_pooling_1d(pool_size = 5) %>% 
  layer_conv_1d(filters = 64, kernel_size = 5, activation = 'relu') %>% 
  layer_max_pooling_1d(pool_size = 35) %>% 
  layer_flatten() %>% 
  layer_dense(units = 64, activation = 'relu') %>% 
  layer_dense(units = president_count, activation = 'softmax')


model <- keras_model(sequence_input, preds)

model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(lr=0.002),
  metrics = c('acc')  
)

model %>% fit(
  x_train, y_train,
  batch_size = 128,
  epochs = 11,
  validation_data = list(x_val, y_val)
) %>% plot()

#Test and Training Accuracies
(test_accuracy_transferlearn = model %>% evaluate(x_val, y_val))$acc
(train_accuracy_transferlearn = model %>% evaluate(x_train, y_train))$acc

#Getting the predictions:
predictions <- model %>% predict(x_val)

range(predictions)


#Getting the max of each rows prediction and binding it on the end
pres.max <- as.data.frame(predictions)
pres.max <- apply(predictions,1,which.max) 
pres.max <- unlist(as.numeric(as.character(pres.max)))
pres.max <- cbind(predictions,pres.max )

pres.actual <- as.data.frame(y_val)
pres.actual <- apply(y_val,1,which.max) 

predictions_val <- cbind(sentence_data$validate,pres.max,pres.actual)

#We can see each presidents' classifications individuallyby filtering
head(predictions_val %>% filter(president == "Mbeki") %>% select(pres.max, pres.actual)) #President numero 1
head(predictions_val %>% filter(president == "Zuma") %>% select(pres.max, pres.actual)) #President numero 2
head(predictions_val %>% filter(president == "Mandela") %>% select(pres.max, pres.actual)) #President numero 3
head(predictions_val %>% filter(president == "Motlanthe") %>% select(pres.max, pres.actual)) #President numero 4
head(predictions_val %>% filter(president == "Ramaphosa") %>% select(pres.max, pres.actual)) #President numero 5
head(predictions_val %>% filter(president == "deKlerk") %>% select(pres.max, pres.actual))#President numero 6

(t = table(pred = predictions_val$pres.max, actual = predictions_val$pres.actual))
paste0("President sentence counts in validation set:")
(table(sentence_data$validate$president))
paste0("Baseline_accuracies")
(baseline_accuracies = table(input_data$sentences$president)*100/sum(table(input_data$sentences$president)))
```
Majority of the sentences of Mbeki is predicted as Mandela(379/569).
Majority of the sentences of Zuma is predicted as Mandela (244/534) and the secon majority is predicted as himself (195/534).
Majority of the sentences of Mandela is predicted as Mandela(263/381).
Majority of the sentences of Mothlane(38/66) is predicted as Mandela.
Majority of sentences of Ramaphosa is predicted as Mandela(19/47) or Zuma(15/47).
Mojority of the sentences of deKlerk is predicted as Mandela(14/17).


```{r}
#Let's calculate Cohen's kappa for this model
library(psych)
cohen.kappa(t) #0.116 #We are doing slightly better than random classifier.
```




## **Neural Net (nn) to predict the President from a Sentence**


```{r nn}

# sam / merve?? 
# merve says: I have actually done my predictions within chunks, so we can remove this part. Votes?

paste0("Test accuracy for the bigger word-count model is ", test_accuracy_wordcount_bigger$acc) #0.55886
paste0("Train accuracy for the bigger word-count model is ", train_accuracy_wordcount_bigger$acc)

paste0("Test accuracy for the smaller word-count model is ", test_accuracy_wordcount_smaller$acc) #0.5811648
paste0("Train accuracy for the smaller word-count model is ", train_accuracy_wordcount_smaller$acc)

paste0("Test accuracy for the tf-idf model is ", test_accuracy_tfidf$acc) #0.6146221
paste0("Train accuracy for the tf-idf model is ", train_accuracy_tfidf$acc)

paste0("Test accuracy for the sentiment analysis nn is ", test_accuracy_sentiment$acc) #
paste0("Train accuracy for the sentiment analysis nn is ", train_accuracy_sentiment$acc)

paste0("Test accuracy for the topic modelling nn is ", test_accuracy_topic$acc) #
paste0("Train accuracy for the topic modelling nn is ", train_accuracy_topic$acc)

paste0("Test accuracy for the transfer learning CNN is ", test_accuracy_transferlearn$acc) #0.4634449
paste0("Train accuracy for the transfer learning CNN is ", train_accuracy_transferlearn$acc)


```

### **Evaluate Out of Sample Performance**

test_acc

## **Convolutional Neural Net (cnn) to predict the President from a Sentence**

```{r cnn}

# ?? Also done this part inside the chunks, thought it was easier to see that way.

```



### **Evaluate Out of Sample Performance**


#### **Recurrent Neural Net (rnn) to predict President from Sentence**


```{r rnn}

# sam

```



#### **Evaluate Out of Sample Performance**

here we can combine all the train and test accuracies in a table, as a conclusion.

## **Analysis of Results and Conclusion**

Also done the critisism after each chunk. 


## **References**

https://www.kaggle.com/rtatman/tutorial-sentiment-analysis-in-r

https://www.datacamp.com/community/tutorials/sentiment-analysis-R

https://nlp.stanford.edu/pubs/glove.pdf 

https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html

https://keras.rstudio.com/articles/examples/pretrained_word_embeddings.html

https://github.com/rstudio/keras/blob/master/vignettes/examples/pretrained_word_embeddings.R

Landis, J.R.; Koch, G.G. (1977). ???The measurement of observer agreement for categorical data???. Biometrics 33 (1): 159???174