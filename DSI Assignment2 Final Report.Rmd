---
title: "DSI Assignment 2 Final Report"
authors: "Merve Aydin Chester AYDMER001",
         "Audrey Pentz, PNTAUD001",
         "Sam Perumal PRMSAM001",
         "Vanessa Saker, SKRVAN001"
date: "September 20, 2018"
output: html_document
---


```{r setup, include=FALSE}

##---------------------------------##
## ADD ANY LIBRARIES USED HERE
##---------------------------------##

knitr::opts_chunk$set(
  echo = TRUE,
  library(tidyverse),
  library(tidytext),
  library(stringr),
  library(lubridate),
  library(knitr),
  library(wordcloud),
  library(wordcloud2),    # wordcloud
  library(kableExtra),    # tables
  library(formattable),    # coloured cell in a table
  library(keras)
  
)

##---------------------------------##
## DATA LOADED SO NO NEED TO RELOAD
##---------------------------------##

load("~/Downloads/dsi-assign2/input_data.RData")
load("~/Downloads/dsi-assign2/sentence_data.RData")
```


# **Description of the Problem**

We were given 30 State of the Nation (SONA) speeches from 1994 to 2018 to analyse. The specific objectives are to: 
1. Infer sentiment and changes in sentiment over time  
2. Describe the topics that emerge  
3. Predict the President from a given sentence of text  
4. Evaluate out of sample performance of the predictions  


# **Approach**  

We collaborated using the following GitHub location: https://github.com/samperumal/dsi-assign2. 

We initially split the work as follows and each of us created a folder with our names to push our work to for others to view:  
- Neural Net - Sam and Merve
- Bag of Words - Merve
- Topic Modelling - Vanessa
- Sentiment Analysis - Audrey

We presented our work to each other and made suggestions for improvement. The initial results from the Neural Net gave a 65% accuracy on the validation set and we wanted to feed the results of the Topic Modelling and Sentiment Analysis into the Neural Net to see if it would improve results so we needed to understand from each other what the output of these 2 methods was and the input required by the neural net to get the data into a useable format which took some discussion and a few iterations.

Given the low accuracy of the neural net (NN), we also tried a Convolutional Neural Net (CNN). Sam got the initial model working. Merve made improvements. Vanessa tuned the hyperparamters.
...need more here...

The cnn did not provide much improvement over the NN so we also tried a Recurrent Neural Net (RNN) which takes in sequences of data so the order of the words is also taken into account in the model.
...need more here...

Initially we each performed our own import of the data, splitting out the year and president and tokenisation but we realised there was duplication of effort here and different naming conventions which made it difficult to collaborate and use each other's output. In addition, Sam noticed that some of the data was not being read in because of special characters and sentences were not being tokenised correctly for various reasons so he became responsible for performing the data clean up (preprocessing) and outputting a .RData file that we could all then use to rerun our work. This is explained in more detail below and at a high level includes:   
- removing special characters such as slashes, currency symbols, currency values and numbers  
- changing bullet points, colons, semi-colons and ellipsis to full stops  
- changing the word after a full stop to sentence case  
- removing multiple spaces and multiple full stops  

Sam also assigned sentence id's using hashing and split the data into training and validation sets so that there would be consistency across what we were working on so that we could use each others work and compare results more easily.

```{r preprocessing}

# sam

```

Before diving into any analysis, we felt it is important to do an Exploratory Data Analysis (EDA) to get a sense of the high level overview of the dataset. This was done by Audrey.

### **Overview of the dataset**  

Each president has made a certain number of SONA speeches, depending on their term in office and whether there was 1 speech that year or 2 in the year of an election (pre and post election). Let's understand the number of words used by each President and how this varies across each SONA speech.  

#### **Average number of words used per President**  

We need to create a metric called "avg_words" which is simply the total number of words across all SONA speeches made by a particular president, divided by the total number of SONA speeches that president made.  

```{r descPres}

# sentence tokenization
tidy_sona_sentences <- input_data$sentences

# word tokenization
tidy_sona_words <- input_data$words %>% 
  filter(!word %in% stop_words$word, str_detect(word, "[a-z]"))  # remove stop words

# bigram tokenization
bigrams <- tidy_sona_sentences %>% 
  unnest_tokens(bigram, sentence, token = "ngrams", n = 2)
# separate the bigrams 
bigrams_separated <- bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")
# remove stop words
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)
# join up the bigrams again
tidy_sona_bigrams <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

# speeches
speech_count <- tidy_sona_words %>%
  group_by(year, president) %>%
  count() %>% 
  group_by(president) %>% 
  summarise(num_speeches = n())

# avg words per president
avg_word_count <- tidy_sona_words %>%
  group_by(president) %>% 
  summarise(num_words = n()) %>% 
  left_join(speech_count) %>% 
  mutate(avg_words = round((num_words/num_speeches),0)) %>% 
  arrange(desc(avg_words))

# plot avg words per president
avg_word_count %>%
  ungroup(avg_words, president) %>%
  mutate(avg_words = color_bar("lightblue")(avg_words)) %>%
  kable("html", escape = FALSE, align = "l", caption = "Average number of words used per President") %>%
  kable_styling(bootstrap_options = 
                  c("striped", "condensed", "bordered"), 
                  full_width = FALSE)

```
  
##### **Insights:**   

On average, Mbeki used the most words in his SONA speeches, followed by Motlanthe and de Klerk used the least. Mandela and Zuma are ranked in the middle of their peers. The current president (Ramaphosa) used fewer words than all of his post 1994 peers.  



#### **Number of words used per SONA**  

```{r descSONA}

# per SONA
word_count <- tidy_sona_words %>%
  group_by(president, year, election) %>%
  summarise(num_words = n()) %>%
  arrange(desc(num_words)) 

# plot words per SONA
word_count %>%
  ggplot(aes(x = as.numeric(year), y = num_words, colour = president)) +
  geom_point() + 
  geom_smooth(method = "loess", aes(colour = president))

```
  
##### **Insights:**   

Of the 3 presidents that have made more than 1 SONA speech, Mbeki used more words on average than both Mandela and Zuma and the variance in the number of words used per SONA speech is also higher for Mbeki. In 2004, which was an election year, the average number of words Mbeki used was lower in both his pre- and post-election speeches. Towards the end of his term, his average number of words also dropped off. The data suggests that perhaps Mbeki's average number of words is correlated with his confidence in being re-elected President.  



### **What are the common words used across all SONA speeches?**  

```{r}

# word cloud for words
word_count <- tidy_sona_words %>%
  count(word, sort = TRUE)

wordcloud2(word_count[1:300, ], size = .6, color='random-dark')

```
  
##### **Insights:**   

....  


### **What are the common bigrams used across all SONA speeches?**  

```{r}

# word cloud for bigrams
bigram_count <- tidy_sona_bigrams %>%
  count(bigram, sort = TRUE)

wordcloud2(bigram_count[1:300, ], size = .8, color='random-dark')

```
  
##### **Insights:**   

....



### **Lexical Diversity per President**  

Lexical diversity refers to the number of unique words used in each SONA.

```{r diversity}

# word tokenization
diversity_per_year <- input_data$words %>%   # don't remove stop words this time
  group_by(president, year) %>% 
  summarise(diversity = n_distinct(word)) %>% 
  arrange(desc(diversity))

diversity_per_year %>%
  ggplot(aes(x = as.numeric(year), y = diversity, colour = president)) +
  geom_point(color = "steelblue",
               alpha = .7,               # transparency
               size = 3,                 # point size
               position = "jitter") +    # point overlap
  geom_smooth(method = "loess", aes(colour = president)) +
  ggtitle("Lexical Diversity per President") +
  xlab("year") +
  ylab("")

```
  
##### **Insights:**   

The number of unique words per SONA ranges from about 700 with de Klerk in 1994 to over 2500 with Mandela in his post election speech of 1999. Mbeki's post election speech of 2004 and Zuma's post election speech of 2014 also got close to the 2500 mark.  

It's interesting that whilst the trend in the number of unique words used was most often upwards with Mandela, Mbeki and Zuma both show a mostly upward trend in the lead up to the election year, followed by a mostly downward trend after nearing the 2500 unique words mark in their post election speech.  

If we exclude the post election speeches, the number of unique words used by Mbeki during his term from 2000 to 2008 averages just under 2000 whereas the number of unique words used by Zuma during his term from 2009 to 2017 averages just over 1500.  



### **Lexical Density per President**  

Lexical density refers to the number of unique words used in each SONA divided by the total number of words and a high value is an indicator of word repitition.  

```{r density}

# word tokenization
density_per_year <- input_data$words %>%   # don't remove stop words this time
  group_by(president, year) %>% 
  summarise(density = n_distinct(word) / n()) %>% 
  arrange(desc(density))

density_per_year %>%
  ggplot(aes(x = as.numeric(year), y = density, colour = president)) +
  geom_point(color = "steelblue",
               alpha = .7,               # transparency
               size = 3,                 # point size
               position = "jitter") +    # point overlap
  geom_smooth(method = "loess", aes(colour = president)) +
  ggtitle("Lexical Density per President") +
  xlab("year") +
  ylab("")

```
  
##### **Insights:**   

De Klerk repeated over 30% of his words in his 1994 pre election SONA speech. On average, Mandela repeated about 25% of words in each of his SONA speeches and this reduced to about 20% in the post election speech of 1999. Mbeki's repitition rate was about 23% and this reduced to 20% in the post election speech of 2004. Zuma's repitition rate is over 30% with the exception of his post election speech of 2014 at about 23%.  



# **Results**


## **Sentiment Analysis**

```{r sentiment}

# audrey

```
## **Bag of Words Neural Net**

```{r}
president_word_count = input_data$sentences %>%
  group_by(president) %>%
  unnest_tokens(word, sentence) %>%
  count(id, word, sort = TRUE) %>%
  ungroup()
  

bag_of_words <- president_word_count %>% 
  select(ID = id, prez = president,  word,n) %>% 
  spread(key = word, value = n, fill = 0)

set.seed(321)
training_ids <- bag_of_words %>% 
  group_by(prez) %>% 
  sample_frac(0.9) %>% 
  ungroup() %>%
  select(ID)

training_sona <- bag_of_words %>% 
  right_join(training_ids, by = "ID") %>%
  select(-ID)

test_sona <- bag_of_words %>% 
  anti_join(training_ids, by = "ID") %>%
  select(-ID)
nrow(bag_of_words)

# Seperate response variable
dim(training_sona) #6626 rows 10626 cols
x_train <- as.matrix(training_sona %>% select(-prez))
x_test <- as.matrix(test_sona %>% select(-prez), ncol = 10625)

#One-hot encoding for response variable
labels_test = test_sona %>% select(prez) %>% unlist()
president_count = labels_test %>% as_tibble() %>% unique() %>% count()
response_tokenizer = text_tokenizer(num_words = president_count + 1)
response_tokenizer$fit_on_texts(labels_test)
# Extract response vector, ignoring first (empty) column
y_test = (response_tokenizer$texts_to_matrix(labels_test, mode = "binary"))[,-1]

labels_train = training_sona %>% select(prez) %>% unlist()
president_count = labels_train %>% as_tibble() %>% unique() %>% count()
response_tokenizer = text_tokenizer(num_words = president_count + 1)
response_tokenizer$fit_on_texts(labels_train)
# Extract response vector, ignoring first (empty) column
y_train = (response_tokenizer$texts_to_matrix(labels_train, mode = "binary"))[,-1]

#Create a model

model <- keras_model_sequential() #Creating an empty sequential model

#Define a model by sequentially adding layers. Add a simple model so it does not overfit.

model %>%
  layer_dense(units = 8, activation = 'relu', input_shape = ncol(x_train)) %>%
  layer_dense(units = 8, activation = 'relu', kernel_regularizer = regularizer_l2(0.01)) %>%
  layer_dense(units = 6, activation = 'softmax')

#See the number of parameteres:
summary(model)

#Compile and fit the model
model %>% compile(
  optimizer = optimizer_rmsprop(lr = 0.003),
  loss = 'categorical_crossentropy',
  metrics = c('accuracy')
)
model %>% fit(x_train, y_train, epochs = 10, batch_size = 32, validation_data = list(x_test, y_test)
) %>% plot()

model %>% evaluate(x_test, y_test, batch_size=32, verbose = 1)

predictions <- model %>% predict(x_test)
predictions2 = model %>% predict_classes(x_test)
range(predictions)
range(predictions2)


#Getting the max of each rows prediction and binding it on the end
pres.max <- as.data.frame(predictions)
pres.max <- apply(predictions,1,which.max) 
pres.max <- unlist(as.numeric(as.character(pres.max)))
pres.max <- cbind(predictions,pres.max )

pres.actual <- as.data.frame(y_test)
pres.actual <- apply(y_test,1,which.max) 
pres.actual = as.data.frame(pres.actual, ncol=1)

predictions_val <- cbind(test_sona, pres.actual, pres.max)
predictions_val = predictions_val %>% select(prez, pres.actual, pres.max)


t <- table(predictions_val$pres.actual, predictions_val_2$pres.max)
paste0("Accuracy rate is:" , sum(diag(t))/sum(t))

```

Note that it is the same accuracy rate as we have found with model %>% evaluate(). 60.84% accuracy just for the word-count model seems good enough. But this model does not consider how important each word is to its corpus. So we should consider a better model to try.

After the fourth iteration validation loss starts increasing which is a sign of overfitting. 

## **Bag of Words NN using tf-idf**

TFIDF is a statisic that shows how important a word is to it's corpus. So if we are feeding NN with TFIDF we are logically expecting the results to be slightly better than the word-cout NN model. 

```{r tfidf}

tidy_sona = input_data$sentences %>%
  unnest_tokens(word, sentence, token = "words") %>% 
  select(id, word, year, president) 

sona_tdf <- tidy_sona %>%
  select(id,word) %>%
  group_by(id,word) %>%
  count() %>%  
  group_by(id) %>%
  mutate(total = sum(n)) %>%
  ungroup()

sona_tf_idf <- sona_tdf %>% 
  bind_tf_idf(word, id, n) # replace with values from tidytext

#Get presidents names where id's match
nm <- c("id", "president")
sona_tf_idf[nm] <- lapply(nm, function(x) df[[x]][match(sona_tf_idf$id, df$id)])


# Spreading the data
tf_idf_wide <- sona_tf_idf %>% 
  select(ID = id, prez = president, word, tf_idf) %>%  # note the change, using tf-idf
  spread(key = word, value = tf_idf, fill = 0)

#Create training anf testing set
set.seed(321)
training_ids <- tf_idf_wide %>% 
  group_by(prez) %>% 
  sample_frac(0.9) %>% 
  ungroup() %>%
  select(ID)

training_sona <- tf_idf_wide %>% 
  right_join(training_ids, by = "ID") %>%
  select(-ID)

test_sona <- tf_idf_wide %>% 
  anti_join(training_ids, by = "ID") %>%
  select(-ID)

# Seperate response variable and normalize the data
dim(training_sona) #6626 rows 10626 cols
x_train <- normalize(as.matrix(training_sona %>% select(-prez))) 
x_test <- normalize(as.matrix(test_sona %>% select(-prez)))

#One-hot encoding for response variable
labels_test = test_sona %>% select(prez) %>% unlist()
president_count = labels_test %>% as_tibble() %>% unique() %>% count()
response_tokenizer = text_tokenizer(num_words = president_count + 1)
response_tokenizer$fit_on_texts(labels_test)
# Extract response vector, ignoring first (empty) column
y_test = (response_tokenizer$texts_to_matrix(labels_test, mode = "binary"))[,-1]

labels_train = training_sona %>% select(prez) %>% unlist()
president_count = labels_train %>% as_tibble() %>% unique() %>% count()
response_tokenizer = text_tokenizer(num_words = president_count + 1)
response_tokenizer$fit_on_texts(labels_train)
# Extract response vector, ignoring first (empty) column
y_train = (response_tokenizer$texts_to_matrix(labels_train, mode = "binary"))[,-1]

#Define the model

model <- keras_model_sequential() #Creating an empty sequential model

#Define a model by sequentially adding layers.

model %>%
  layer_dense(units = 8, activation = 'relu', input_shape = ncol(x_train)) %>%
  
  layer_dense(units = 8, activation = 'relu', kernel_regularizer = regularizer_l2(0.01)) %>%
  layer_dense(units = 6, activation = 'softmax') %>%
  compile(
  optimizer = optimizer_rmsprop(lr=0.003),
  loss = 'categorical_crossentropy',
  metrics = c('accuracy')
)

model %>% fit(x_train, y_train, epochs = 10, batch_size = 32, validation_data = list(x_test, y_test)
) %>% plot()

model %>% evaluate(x_test, y_test, batch_size=32, verbose = 1) #0.6183395 accuracy rate


predictions <- model %>% predict(x_test)
predictions2 = model %>% predict_classes(x_test)
range(predictions)
range(predictions2)


#Getting the max of each rows prediction and binding it on the end
pres.max <- as.data.frame(predictions)
pres.max <- apply(predictions,1,which.max) 
pres.max <- unlist(as.numeric(as.character(pres.max)))
pres.max <- cbind(predictions,pres.max )

pres.actual <- as.data.frame(y_test)
pres.actual <- apply(y_test,1,which.max) 
pres.actual = as.data.frame(pres.actual, ncol=1)

predictions_val <- cbind(test_sona, pres.actual, pres.max)
predictions_val = predictions_val %>% select(prez, pres.actual, pres.max)


t <- table(predictions_val$pres.actual, predictions_val_2$pres.max)
paste0("Accuracy rate is:" , sum(diag(t))/sum(t))

```
This gives us exactly the same results as the word-count model. But the algorithm learns after
###GloVe word embeddings

We will be using GloVe embeddings. "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space." 
GloVe stands for "Global Vectors for Word Representation". It's a popular embedding technique.
[Reference:Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation:https://nlp.stanford.edu/pubs/glove.pdf ]
Specifically, we will use the 100-dimensional GloVe embeddings of 400k words computed on a 2014 dump of English Wikipedia. 
reference: https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html
reference: https://keras.rstudio.com/reference/save_text_tokenizer.html
```{r}
library(keras)

#Settings for gloves embedding
GLOVE_DIR <- 'glove.6B'
MAX_SEQUENCE_LENGTH <- 1000
MAX_NUM_WORDS <- 40000
EMBEDDING_DIM <- 100
VALIDATION_SPLIT <- 0.2

# download data if necessary
download_data <- function(data_dir, url_path, data_file) {
  if (!dir.exists(data_dir)) {
    download.file(paste0(url_path, data_file), data_file, mode = "wb")
    if (tools::file_ext(data_file) == "zip")
      unzip(data_file, exdir = tools::file_path_sans_ext(data_file))
    else
      untar(data_file)
    unlink(data_file)
  }
}
download_data(GLOVE_DIR, 'http://nlp.stanford.edu/data/', 'glove.6B.zip')

# first, build index mapping words in the embeddings set
# to their embedding vector

cat('Indexing word vectors.\n')

embeddings_index <- new.env(parent = emptyenv())
lines <- readLines(file.path(GLOVE_DIR, 'glove.6B.100d.txt'))
for (line in lines) {
  values <- strsplit(line, ' ', fixed = TRUE)[[1]]
  word <- values[[1]]
  coefs <- as.numeric(values[-1])
  embeddings_index[[word]] <- coefs
}

cat(sprintf('Found %s word vectors.\n', length(embeddings_index)))

# second, prepare text samples and their labels
cat('Tokenizing sona sentences\n')


# finally, vectorize the text samples into a 2D integer tensor
tokenizer <- text_tokenizer(num_words=MAX_NUM_WORDS)
tokenizer %>% fit_text_tokenizer(input_data$sentences$sentence)

# save the tokenizer in case we want to use it again
# for prediction within another R session, see:
# 
#https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html
save_text_tokenizer(tokenizer, "tokenizer")

# If you want to use the same text tokenizer use this:
# load_text_tokenizer("tokenizer")

sequences_train = texts_to_sequences(tokenizer, balanced_train_data$sentence)
sequences_valid = texts_to_sequences(tokenizer, sentence_data$validate$sentence)

word_index <- tokenizer$word_index
cat(sprintf('Found %s unique tokens.\n', length(word_index)))

x_train <- pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)
x_val <- pad_sequences(sequences_valid, maxlen=MAX_SEQUENCE_LENGTH)

# Fit president tokenizer
president_count = balanced_train_data$president %>% as_tibble() %>% unique() %>% count()
response_tokenizer = text_tokenizer(num_words = president_count + 1)
response_tokenizer$fit_on_texts(balanced_train_data$president)
response_tokenizer$fit_on_texts(sentence_data$validate$president )

# One-hot encode president
# Extract response vector, ignoring first (empty) column
y_train = (response_tokenizer$texts_to_matrix(balanced_train_data$president, mode = "binary"))[,-1]
y_val = (response_tokenizer$texts_to_matrix(sentence_data$validate$president , mode = "binary"))[,-1]

cat('Shape of data tensor: ', dim(data_train), '\n')
cat('Shape of data tensor: ', dim(data_valid), '\n')
cat('Shape of label tensor: ', dim(y_train), '\n')
cat('Shape of data tensor: ', dim(y_test), '\n')


# prepare embedding matrix
num_words <- min(MAX_NUM_WORDS, length(word_index) + 1)
prepare_embedding_matrix <- function() {
  embedding_matrix <- matrix(0L, nrow = num_words, ncol = EMBEDDING_DIM)
  for (word in names(word_index)) {
    index <- word_index[[word]]
    if (index >= MAX_NUM_WORDS)
      next
    embedding_vector <- embeddings_index[[word]]
    if (!is.null(embedding_vector)) {
      # words not found in embedding index will be all-zeros.
      embedding_matrix[index,] <- embedding_vector
    }
  }
  embedding_matrix
}

embedding_matrix <- prepare_embedding_matrix()

# load pre-trained word embeddings into an Embedding layer
# note that we set trainable = False so as to keep the embeddings fixed
embedding_layer <- layer_embedding(
  input_dim = num_words,
  output_dim = EMBEDDING_DIM,
  weights = list(embedding_matrix),
  input_length = MAX_SEQUENCE_LENGTH,
  trainable = FALSE
)

cat('Training model\n')

# train a 1D convnet with global maxpooling
sequence_input <- layer_input(shape = list(MAX_SEQUENCE_LENGTH), dtype='int32')

preds <- sequence_input %>%
  embedding_layer %>% 
  layer_conv_1d(filters = 64, kernel_size = 5, activation = 'relu',
                use_bias = TRUE, kernel_initializer = "glorot_uniform") %>% 
  layer_dropout(0.2) %>%
  layer_max_pooling_1d(pool_size = 5) %>% 
  layer_conv_1d(filters = 64, kernel_size = 5, activation = 'relu') %>% 
  layer_max_pooling_1d(pool_size = 5) %>% 
  layer_conv_1d(filters = 64, kernel_size = 5, activation = 'relu') %>% 
  layer_max_pooling_1d(pool_size = 35) %>% 
  layer_flatten() %>% 
  layer_dense(units = 64, activation = 'relu') %>% 
  layer_dense(units = president_count, activation = 'softmax')


model <- keras_model(sequence_input, preds)

model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(lr=0.002),
  metrics = c('acc')  
)

model %>% fit(
  x_train, y_train,
  batch_size = 128,
  epochs = 15,
  validation_data = list(x_val, y_val)
) %>% plot()

#Since the validation loss is still decreasing and the accuracy was increasing we will run 5 more epochs
model %>% 
  fit(x_train, y_train, epochs = 5, batch_size = 128,validation_data = list(x_val, y_val)) %>% 
  plot()

model %>% evaluate(x_val, y_val)
#The accuracy results are improved by 2% when using the balanced data set(see dsi-assign2/merve/pre-trained-embeddings-final), so this says that there might be a better way to balance the data like **smote** 
#The accuracy of the 0.4479554
#As mentioned in the important note part in Keras' website, this code mimics the python code but it is half as successful, so they are pointing this issue there. See: https://github.com/rstudio/keras/blob/master/vignettes/examples/pretrained_word_embeddings.R


predictions <- model %>% predict(x_val)
range(predictions)


#Getting the max of each rows prediction and binding it on the end
pres.max <- as.data.frame(predictions)
pres.max <- apply(predictions,1,which.max) 
pres.max <- unlist(as.numeric(as.character(pres.max)))
pres.max <- cbind(predictions,pres.max )

pres.actual <- as.data.frame(y_val)
pres.actual <- apply(y_val,1,which.max) 


predictions_val <- cbind(sentence_data$validate,pres.max,pres.actual)
View(head(predictions_val, n= 100))
predictions_val %>% filter(president == "Mbeki")

t = table(predictions_val$pres.max, predictions_val$pres.actual)


```



## **Topic Modelling**

```{r topic}

# vanessa

```



## **Neural Net (nn) to predict the President from a Sentence**


```{r nn}

# sam / merve??

```



### **Evaluate Out of Sample Performance**



## **Convolutional Neural Net (cnn) to predict the President from a Sentence**

```{r cnn}

# ??

```



### **Evaluate Out of Sample Performance**


#### **Recurrent Neural Net (rnn) to predict President from Sentence**


```{r rnn}

# sam

```



#### **Evaluate Out of Sample Performance**



## **Analysis of Results and Conclusion**



## **References**

https://www.kaggle.com/rtatman/tutorial-sentiment-analysis-in-r

https://www.datacamp.com/community/tutorials/sentiment-analysis-R



